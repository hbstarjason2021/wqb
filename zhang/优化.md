你现在的“指标差”，本质不是回测/相关性调用方式的问题，而是**生成端的搜索策略几乎等价于“随机写公式”**：一次性让 LLM吐 10 条、没有“可学习的反馈闭环”、也没有把 Brain 的**真实评分函数**（Sharpe/Fitness/约束测试/可投资性/子样本）变成“优化目标”。这跟主流论文里的做法差别很大。

下面我先用你跑出来的结果举证，再对照主流论文，总结它们解决“指标差”的关键机制，最后给你一套我认为**在 Brain-MCP 黑盒评估条件下的最佳可落地方案**（不是泛泛建议，是能直接改成系统的方案）。

------

## 1) 你当前“指标差”的直接证据：信号本体弱 + 风险/可投资性拖累 + 没有优化闭环

### 1.1 你确实生成出了可跑的 alpha，但 Sharpe/Fitness 极低

例如 `zqXmEkgX` 的表达式（model110 的 growth/tree 等组合）回测出来：

- IS（train）Sharpe 只有 **-0.07**、Fitness **-0.01**
- 但 *riskNeutralized* Sharpe 却有 **0.34**（说明剥离风险暴露后反而变“像个信号”）
- test 期 Sharpe **-0.39**，riskNeutralized test Sharpe **0.36**（同样现象）

这类结构非常典型：**你生成的东西更像“风险暴露 + 噪声”，不是“稳健 alpha”。**
主流论文几乎都在“显式处理相关性/有效性/风险暴露/复杂度”上投入了机制（下面展开）。

再看 `1Y6dRwOz`：

- Sharpe **0.06**、Fitness **0.01**（几乎为 0）
- investabilityConstrained Sharpe **-0.48**（可投资性约束后更差）
- 并且直接被 Brain 的 `LOW_SHARPE` / `LOW_FITNESS` 等测试 FAIL

你现在“指标差”不是偶发，是系统性地在生成**弱信号**。

------

### 1.2 生成策略是“一次性作文式生成”，没有任何基于回测反馈的优化链

你的生成 prompt 是典型的“一次性给 hypothesis + operators_reference + dataset_reference，然后要 10 条 diverse alpha”。

这会导致两个结果：

1. **探索效率极低**：10 条里想碰到 Sharpe>1（更别说 1.58）几乎靠运气。
2. **不学习**：哪怕某条 alpha 在 riskNeutralized 下表现好，系统也不会“沿着这个方向做去风险修正/参数扫描/结构变异”。

主流方法最核心的提升就在这里：

> **把 alpha 挖掘变成“搜索 + 反馈驱动的迭代优化”**，而不是一次性生成。

------

## 2) 主流论文怎么解决“指标差”：它们不是更会写公式，而是更会“优化”

我按“你最需要借鉴的机制”来对照几篇代表作：

### 2.1 AlphaGen（KDD 2023）：奖励不是“单条 Sharpe”，而是“对组合/集合的增益”

AlphaGen 的核心思想是：生成 alpha 时考虑 **与已有 alpha 集合的协同**，奖励可以定义为对组合模型/集合性能的边际贡献，从而驱动模型产生“更有用、更多样”的 alpha，而不是重复的弱 alpha。([arXiv](https://arxiv.org/abs/2306.12964?utm_source=chatgpt.com))

你能借鉴的点（在 Brain 里可落地）：

- 把 reward 从 `Sharpe(alpha)` 改成：
  - `Score(alpha) - λ * maxCorr(alpha, pool)`
  - 或者“加入 alpha pool 后，pool 的综合评分提升多少”（近似 AlphaGen 的边际贡献思想）

> 只看单条 Sharpe，会让生成器反复碰壁；加入“协同/去相关”会显著提高有效探索效率。

------

### 2.2 Alpha²（2024）：把公式构造当“程序构建”，做 **合法性/维度检查 + 相关性与多样性指标进入评价**

Alpha² 把 alpha 发现表述为“逐步构建程序（表达式）”，并强调两个很现实的点：

1. **有效性/逻辑检查（例如维度分析）**，用于剪枝；
2. 评价指标鼓励 **性能 + 多样性**，并把相关性问题纳入框架。([arXiv](https://arxiv.org/abs/2406.16505?utm_source=chatgpt.com))

你能借鉴的点：

- 不要让 LLM直接吐最终表达式；改成“逐步扩展 AST/语法树”，每一步只允许合法 action（操作符、字段、窗口）。
- 对每个“部分表达式”用快速规则剪枝（复杂度/窗口/是否一定会导致高换手/极端值等），减少无效回测。

------

### 2.3 Alpha-GPT（2023）：人机交互的关键是“用反馈更新搜索方向”，而不是“写得像研究员”

Alpha-GPT 的要点在于它把 LLM 放进一个“交互式 alpha mining 框架”，利用反馈迭代生成。([arXiv](https://arxiv.org/abs/2308.00016?utm_source=chatgpt.com))

你能借鉴的点：

- 你现在只有“生成链”，缺“反馈驱动的优化链”（下面 Chain-of-Alpha 会更贴近你要做的全自动）。

------

### 2.4 Chain-of-Alpha（2025）：双链结构（生成链 + 优化链），用回测反馈做“自动修正/微调”

Chain-of-Alpha 明确提出：用 LLM 做 **全自动** 公式 alpha 挖掘，并用“Factor Generation Chain + Factor Optimization Chain”反复迭代，利用回测反馈和历史优化知识来提高质量与效率。([arXiv](https://arxiv.org/abs/2508.06312?utm_source=chatgpt.com))

这对你当前问题（指标差）几乎是“对症下药”：

- 你现在：只会出候选，不会优化
- 它：把“优化”当成同等重要的链路

------

### 2.5 AlphaAgent（2025）：用正则化解决“同质化/过拟合/alpha decay”

AlphaAgent 指出 LLM 挖 alpha 常见问题：过度依赖已有知识导致同质化，进而加速拥挤与 alpha 衰减；它用三类机制：

- **原创性约束**（AST 相似度惩罚）
- **假设-因子一致性**（语义对齐）
- **复杂度控制**（结构约束，避免过工程化）([arXiv](https://arxiv.org/abs/2502.16789?utm_source=chatgpt.com))

你能借鉴的点：

- 你现在让 LLM“多样化”，但没有“可度量的多样化”；
- AlphaAgent 用 AST/结构相似度，**可计算、可约束**。

------

（补充：如果你想系统性梳理这类方法，2025 的综述把 LLM alpha mining 从 agent 视角做了结构化归类，也很适合你做系统设计时当路线图。）([Journal Publisher | Academax](https://www.academax.com/doi/10.1631/FITEE.2500386?utm_source=chatgpt.com))

------

## 3) 在 Brain-MCP 黑盒回测条件下，我给你的“最佳方案”

你现在的约束是：

- 你可以用 Brain MCP 做回测、拿 tests、拿相关性；
- 但你没有本地全量数据来做廉价评估（所以每次评估都贵）。

因此“最佳方案”的核心目标是：**用尽可能少的 Brain 回测次数，得到尽可能高的 Sharpe/Fitness**。

我建议采用：

> **Chain-of-Alpha 的双链迭代**（生成链 + 优化链）
>
> - **AlphaAgent 的三类正则化**（原创性/一致性/复杂度）
> - **Alpha² / AlphaGen 的多目标奖励与去相关机制**（性能+多样性、协同）

### 3.1 定义一个真正能驱动“指标变好”的评分函数（Reward / Score）

不要只看 `train.sharpe`，要把 Brain 的“会让你 FAIL 的东西”纳入目标。

建议你的核心 score 用这几项（你已经能从 simulation 返回拿到）：

- `S_test = test.sharpe`（更接近泛化）
- `S_train = train.sharpe`（用于稳定性，但权重小）
- `F = train.fitness` 或 Brain 输出的 fitness
- `IC = investabilityConstrained.sharpe`（你这里很多都被它拖垮）
- `T = turnover`（用软惩罚，不要硬卡死）
- `C_prod = max production correlation`（MCP可查）
- `C_self = self correlation`（MCP可查）

一个可落地的打分例子：

[
Score = 0.55,S_{test} + 0.25,S_{train} + 0.20,F ;-; 0.30,\max(0,C_{prod}-0.7) ;-; 0.15,Penalty_{turnover} ;-; 0.20,Penalty_{investability}
]

> 关键不是系数精确，而是：**你要让系统“优化你真正关心的约束下的 Sharpe/Fitness”，而不是优化一个看起来好看的单指标。**

------

### 3.2 第一阶段：Field Screening（字段/原子信号筛选）——把随机搜索变成“带先验的搜索”

在 Alpha²/AlphaGen 这类工作里，一个隐含但非常关键的工程事实是：

> **如果底层 operand（字段）本身几乎没有信息含量，再花哨的算子堆叠也很难救。**([arXiv](https://arxiv.org/abs/2406.16505?utm_source=chatgpt.com))

你现在直接让 LLM在 `model110` 字段里“自由发挥”，等价于没先验。

**做法（非常建议）：**

- 对 `model110` 的每个字段（或你语义匹配 hypothesis 的一小批字段）跑一组固定模板（每个字段几十次以内）：
  - `rank(field)`
  - `rank(ts_mean(field, n))`
  - `rank(ts_delta(field, n))`
  - `rank(ts_rank(field, n))`
  - `-rank(ts_delta(field, n))`
  - n ∈ {5, 10, 20, 60, 120}
- 用上面的 `Score()` 给每个字段打分
- 取 top-K 字段作为下一阶段的“白名单 operand”

这一步会让你后面生成的表达式 **平均质量提高一个数量级**（因为从“随机字段”变成“高潜字段”）。

------

### 3.3 第二阶段：Template + Local Search（模板化组合 + 局部优化）——比“重新写一条”有效得多

你现在失败的 alpha 里很大一部分是“结构看起来合理，但参数/组合方式不对”。比如：

- 风险中性后好，但原始差 → 说明需要“去风险暴露/做中性化/改组合权重”
- investabilityConstrained 很差 → 说明需要降低集中度/提高可投资性

主流方法（尤其 Chain-of-Alpha）强调 **优化链**：基于反馈做“局部改写”，而不是每次从零写。([arXiv](https://arxiv.org/abs/2508.06312?utm_source=chatgpt.com))

**建议你把每个候选 alpha 的优化拆成 4 个“确定性可搜索”的维度：**

1. **符号翻转 / 单调变换**

- `alpha` vs `-alpha`
- `rank(alpha)` vs `zscore(alpha)`
- `clip/winsorize`（Brain 有 truncation；表达式里也可能有 clamp 类算子）

1. **窗口扫描（最常见的收益来源）**

- 对所有 lookback：在 {5,10,20,40,60,120,252} 做网格
- 同一结构，只改窗口，往往能从 Sharpe 0.2 拉到 1.0+（当然不保证，但这是最便宜的提升方式）

1. **结构保持的变异（AlphaAgent 提倡的“复杂度控制”也能融进去）**

- 保持 AST 深度不变：只替换一个 operator / field
- 保持含义：例如“动量/波动归一”结构不变，只替换“波动定义/平滑方式”

1. **settings 小扫（中性化/decay/truncation）**
   你的 settings 固定为 `neutralization="SECTOR", decay=4, truncation=0.02`
   但从你结果看，“riskNeutralized 能救回来”说明 settings 可能不是最优。
   建议扫：

- neutralization ∈ {SECTOR, INDUSTRY, NONE}
- decay ∈ {0,2,4,8}
- truncation ∈ {0.01,0.02,0.05}

------

### 3.4 第三阶段：双链 LLM（生成链 + 优化链）——用回测反馈让 LLM“学会改进”

这一步是你当前最缺的，也是 Chain-of-Alpha 的核心。([arXiv](https://arxiv.org/abs/2508.06312?utm_source=chatgpt.com))

#### 生成链（Generation Chain）

输入：

- hypothesis
- top-K fields（来自筛选）
- 允许的模板集合（30~50 个）
- 结构约束：AST 深度≤6、运算符数≤10、禁止某些高风险算子组合

输出：

- 只输出**结构**（JSON DSL / AST），不要直接输出最终字符串（避免胡写）

#### 优化链（Optimization Chain）

输入：

- alpha 表达式
- Brain 回测摘要（train/test/IC/turnover/失败 tests 名称）
- 与 alpha pool 的相关性（prod/self/pool corr）
- “历史优化记忆”（哪些改动通常提升 test sharpe、哪些经常变差）

输出：

- 生成一组“局部改写候选”（通常 5~20 个），优先改：
  1. 导致 FAIL 的项（LOW_SHARPE、LOW_FITNESS、CONCENTRATED_WEIGHT…）
  2. investabilityConstrained 很差的项
  3. riskNeutralized 好但原始差的项（提示要去风险）

> 你现在看到的现象（riskNeutralized 明显好于原始）就是优化链最容易“吃到红利”的地方：它告诉你“正确方向在哪”，但你系统没有利用。
> 例如 `zqXmEkgX` 这种情况就应该进入“去风险暴露/中性化强化/结构调整”的优化队列。

------

### 3.5 加入 AlphaAgent 式正则化：让探索更“有效且不内卷”

AlphaAgent 的三条机制，我建议你都上（都能在 Brain 黑盒下实现）：([arXiv](https://arxiv.org/abs/2502.16789?utm_source=chatgpt.com))

1. **原创性惩罚（AST similarity penalty）**

- 你维护一个 alpha pool（已回测过的表达式的 AST）
- 新候选与 pool 的相似度高 → 直接拒绝或降权
  （这会显著减少你“看似不同、实则同质”的重复探索）

1. **假设一致性（Hypothesis–Factor alignment）**

- 让 LLM 给每条 alpha 输出“它对应 hypothesis 的哪一部分”（growth / volatility / interaction / regime）
- 再让一个 critic LLM 判定“是否真的对应”，不对应则拒绝
  （避免 LLM为了多样性乱写不相关的东西）

1. **复杂度控制（Complexity constraint）**

- 硬约束 AST 深度、运算符个数、窗口个数
- 超复杂的表达式通常过拟合/不稳健，且更容易在 investability/集中度上翻车

------

## 4) 一句话“最佳路线图”

如果你只选一条路线来把指标做上去，我建议你按这个顺序实现（每一步都能立刻改善平均指标）：

1. **字段筛选（Field screening）**：先找到“有信息含量”的 operands
2. **模板化生成（Template bank + DSL）**：降低随机性，提升可控性
3. **优化链（Chain-of-Alpha style）**：用回测反馈做局部改写 + 参数/设置扫描
4. **多目标 reward（AlphaGen/Alpha² style）**：把相关性、可投资性、子样本等纳入目标
5. **正则化探索（AlphaAgent style）**：用 AST 相似度/复杂度/假设一致性提升探索效率和多样性

这套组合基本就是把“论文里真的有效的机制”翻译成“你现在 Brain MCP 能跑起来的工程版本”。([arXiv](https://arxiv.org/abs/2508.06312?utm_source=chatgpt.com))

------

## 5) 你现在代码层面最关键的“生成逻辑”该怎么改（具体到你这份实现）

你现在的生成函数是把 dataset_reference 和 operators_reference 全量塞给 LLM，然后让它写 10 个表达式。

**建议改成：**

### 改法 A（最快见效）：两段式 prompt + 强约束输出

- Prompt1：让 LLM 只做“字段选择 + 模板选择 + 结构草图（JSON）”
- 你程序把 JSON 渲染成表达式
- Prompt2：拿回测结果，让 LLM 只做“局部改写候选”（禁止从零重写）

### 改法 B（更像论文路线）：Beam Search / MCTS-lite

- 用 grammar 把表达式当 AST 构建
- 每扩展一层，保留 top-N（按 surrogate score 或历史统计）
- 只对 top-N 的最终表达式做 Brain 回测
  （这是 Alpha²“程序构建 + 剪枝”的工程简化版）([arXiv](https://arxiv.org/abs/2406.16505?utm_source=chatgpt.com))

------

如果你愿意，我可以直接按你当前 repo 的结构给你一个“可直接替换”的实现方案（包含伪代码/数据结构）：

- `score(sim_result) -> float`（把 Brain 返回的 train/test/investability/tests/corr 统一成 reward）
- `field_screening()`（批量跑模板得到 top fields）
- `generate_chain()`（输出 JSON AST）
- `optimize_chain()`（读回测反馈 → 生成局部改写）
- `search_loop()`（budget 控制：每轮多少候选、多少优化步、如何淘汰）

你只要回我两个信息就够（不需要长解释）：

1. 你目标是优先冲 **单条提交通过（Sharpe/Fitness 门槛）**，还是优先建 **低相关 alpha 库**？
2. 你现在每轮能承受大概多少次 Brain 回测（比如 200 / 2000 / 20000）？

我会按你的预算给出“最优的搜索策略参数”（beam 宽度、优化步数、筛选阈值等）。
