import sys

import time

from datetime import datetime

import pandas as pd

import os

import threading

import concurrent.futures

import requests

from requests.exceptions import RequestException, ConnectionError

import json

import argparse

import queue

import re # Added for modify_alpha_expression




# --- Global Variables and Constants (from machine_lib.py and optimizeAlpha.py) ---

brain_api_url = os.environ.get("BRAIN_API_URL", "https://api.worldquantbrain.com")




UNIVERSE_DICTS = {

    "USA": ["TOP3000", "TOP1000", "TOP500", "TOP200", "ILLIQUID_MINVOL1M", "TOPSP500"],

    "GLB": ["TOP3000", "MINVOL1M","TOPDIV3000"],

    "EUR": ["TOP2500", "TOP1200", "TOP800", "TOP400", "ILLIQUID_MINVOL1M"],

    "ASI": ["MINVOL1M", "ILLIQUID_MINVOL1M"],

    "CHN": ["TOP2000U"],

    "AMR": ["TOP600"],

    "IND": ["TOP500"]

}




NEUT_DICTS = {

    'USA': ['REVERSION_AND_MOMENTUM','STATISTICAL','CROWDING', 'FAST', 'SLOW_AND_FAST'],

    'GLB': ['REVERSION_AND_MOMENTUM','STATISTICAL','CROWDING', 'FAST'],

    'EUR': ['REVERSION_AND_MOMENTUM','STATISTICAL','CROWDING', 'FAST', 'SLOW_AND_FAST'],

    'ASI': ['REVERSION_AND_MOMENTUM','STATISTICAL','CROWDING', 'FAST', 'SLOW_AND_FAST'],

    'CHN': ['REVERSION_AND_MOMENTUM','STATISTICAL','CROWDING', 'FAST', 'SLOW_AND_FAST'],

    'KOR': ['MARKET', 'SECTOR', 'INDUSTRY', 'SUBINDUSTRY'],

    'TWN': ['MARKET', 'SECTOR', 'INDUSTRY', 'SUBINDUSTRY'],

    'HKG': ['MARKET', 'SECTOR', 'INDUSTRY', 'SUBINDUSTRY'],

    'JPN': ['MARKET', 'SECTOR', 'INDUSTRY', 'SUBINDUSTRY'],

    'AMR': ['MARKET', 'SECTOR', 'INDUSTRY', 'SUBINDUSTRY', 'COUNTRY'],

    'IND': ['REVERSION_AND_MOMENTUM','CROWDING', 'FAST', 'MARKET', 'SECTOR', 'INDUSTRY', 'SUBINDUSTRY']

}




file_lock = threading.Lock()




# --- Utility Functions (from machine_lib.py and optimizeAlpha.py) ---




def login():

    # ä»txtæ–‡ä»¶è§£å¯†å¹¶è¯»å–æ•°æ®

    # txtæ ¼å¼:

    # password: 'password'

    # username: 'username'

    def load_decrypted_data(txt_file='user_info.txt'):

        try:

            with open(txt_file, 'r') as f:

                data = f.read()

                data = data.strip().split('\n')




                data = {line.split(': ')[0]: line.split(': ')[1] for line in data}




            return data['username'][1:-1], data['password'][1:-1]

        except FileNotFoundError:

            print(f"Error: {txt_file} not found. Please create it with 'username: 'your_username'\npassword: 'your_password'.")

            sys.exit(1)

        except Exception as e:

            print(f"Error loading user info from {txt_file}: {e}")

            sys.exit(1)




    username, password = load_decrypted_data("user_info.txt")




    # Create a session to persistently store the headers

    s = requests.Session()




    # Save credentials into session

    s.auth = (username, password)




    # Send a POST request to the /authentication API

    try:

        response = s.post(f'{brain_api_url}/authentication')

        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)

        print("Authentication successful.")

    except RequestException as e:

        print(f"Authentication failed: {e}")

        sys.exit(1)

    return s




def set_alpha_properties(

        s,

        alpha_id,

        name: str = None,

        color: str = None,

        selection_desc: str = None,

        combo_desc: str = None,

        tags: list = None,  # ['tag1', 'tag2']

):

    """

    Function changes alpha's description parameters (with 3 retries)

    """




    if alpha_id is None:

        print("Alpha ID ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œå±æ€§æ›´æ–°ã€‚")

        return False

    max_retries = 3

    params = {

        "category": None,

        "regular": {"description": None},

    }

    if color:

        params["color"] = color

    if name:

        params["name"] = name

    if tags:

        params["tags"] = tags

    if combo_desc:

        params["combo"] = {"description": combo_desc}

    if selection_desc:

        params["selection"] = {"description": selection_desc}




    for retry in range(max_retries):

        try:

            response = s.patch(

                f"{brain_api_url}/alphas/{alpha_id}", json=params

            )

            # æ£€æŸ¥å“åº”çŠ¶æ€ç æ˜¯å¦ä¸ºæˆåŠŸï¼ˆ2xxï¼‰

            if 200 <= response.status_code < 300:

                print(f"æˆåŠŸè®¾ç½® alpha_id: {alpha_id}, æ ‡ç­¾: {tags if tags else 'æ— '}ï¼ˆç¬¬ {retry + 1}/{max_retries} æ¬¡å°è¯•ï¼‰")

                return response  # æˆåŠŸåˆ™è¿”å›å“åº”

            else:

                if response.status_code == 429:

                    print(f"è¯·æ±‚è¿‡å¤šï¼ˆ429ï¼‰ï¼Œå°è¯•é‡æ–°ç™»å½•...ï¼ˆç¬¬ {retry + 1}/{max_retries} æ¬¡å°è¯•ï¼‰")

                    s=login()

                print(f"è¯·æ±‚å¤±è´¥ï¼ˆçŠ¶æ€ç : {response.status_code}ï¼‰ï¼Œalpha_id: {alpha_id}ï¼ˆç¬¬ {retry + 1}/{max_retries} æ¬¡å°è¯•ï¼‰")

        except Exception as e:

            print(f"è¯·æ±‚å¼‚å¸¸: {str(e)}ï¼Œalpha_id: {alpha_id}ï¼ˆç¬¬ {retry + 1}/{max_retries} æ¬¡å°è¯•ï¼‰")

        

        # éæœ€åä¸€æ¬¡é‡è¯•æ—¶ç­‰å¾…1ç§’

        if retry < max_retries - 1:

            time.sleep(1)




    # æ‰€æœ‰é‡è¯•å‡å¤±è´¥

    print(f"ä¸‰æ¬¡é‡è¯•å‡å¤±è´¥ï¼Œalpha_id: {alpha_id}")

    return None




def get_alpha_byid(s, alpha_id):

    # ä¸ºAPIè¯·æ±‚æ·»åŠ è¶…æ—¶ï¼Œé˜²æ­¢é•¿æ—¶é—´é˜»å¡

    request_timeout = 60 # 60ç§’è¶…æ—¶

    while True:

        try:

            alpha = s.get(f"{brain_api_url}/alphas/{alpha_id}", timeout=request_timeout)

            if "retry-after" in alpha.headers:

                time.sleep(float(alpha.headers["Retry-After"]))

            else:

                alpha.raise_for_status() # æ£€æŸ¥HTTPçŠ¶æ€ç 

                break

        except requests.exceptions.Timeout:

            print(f"è¯·æ±‚ alpha_id={alpha_id} çš„ '/alphas' æ¥å£è¶…æ—¶ã€‚")

            time.sleep(5) # çŸ­æš‚ç­‰å¾…åé‡è¯•

            s=login() # å°è¯•é‡æ–°ç™»å½•

        except requests.exceptions.RequestException as e:

            print(f"è¯·æ±‚ alpha_id={alpha_id} çš„ '/alphas' æ¥å£å‘ç”Ÿé”™è¯¯: {e}")

            time.sleep(5) # çŸ­æš‚ç­‰å¾…åé‡è¯•

            s=login() # å°è¯•é‡æ–°ç™»å½•

    string = alpha.content.decode('utf-8')

    metrics = json.loads(string)

    return metrics




def write_to_file(alpha, name):

    with file_lock:

        try:

            os.makedirs('records', exist_ok=True)

            # Ensure alpha is stripped of any leading/trailing whitespace, including newlines

            cleaned_alpha = alpha.strip()

            with open(f'records/{name}_simulated_alpha_expression.txt', mode='a') as f:

                f.write(cleaned_alpha + '\n')

                f.flush()

                print(f"Alpha expression written to file: {cleaned_alpha}")

        except Exception as e:

            print(f"å†™å…¥æ–‡ä»¶æ—¶å‡ºé”™: {e}")




class SessionManager:

    def __init__(self, session, start_time, expiry_time):

        self.session = session

        self.start_time = start_time

        self.expiry_time = expiry_time

        self.lock = threading.Lock()  # æ·»åŠ çº¿ç¨‹é”ä¿æŠ¤sessionåˆ·æ–°

        self.needupdate = False # Add this attribute for consistency




    def refresh_session(self):

        with self.lock:  # ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤sessionåˆ·æ–°è¿‡ç¨‹

            print("Session expired, logging in again...")

            if self.session:

                self.session.close()

            self.session = login()  # ä½¿ç”¨åŒæ­¥loginå‡½æ•°

            self.start_time = time.time()

            self.needupdate = False # Reset after refresh




def locate_details(s, alpha_id):

    while True:

        alpha = s.get(f"{brain_api_url}/alphas/" + alpha_id)

        if "retry-after" in alpha.headers:

            time.sleep(float(alpha.headers["Retry-After"]))

        else:

            break

    string = alpha.content.decode('utf-8')

    metrics = json.loads(string)




    # ä½¿ç”¨ get æ–¹æ³•å®‰å…¨è·å–æ•°æ®

    is_data = metrics.get("is", {})

    sharpe = is_data.get("sharpe", 0.0)

    fitness = is_data.get("fitness", 0.0)

    turnover = is_data.get("turnover", 0.0)

    margin = is_data.get("margin", 0.0)

    

    settings = metrics.get("settings", {})

    decay = settings.get("decay", 0)

    delay = settings.get("delay", 0)

    exp = metrics.get('regular', {}).get('code', "")

    universe = settings.get("universe", "")

    truncation = settings.get("truncation", 0)

    neutralization = settings.get("neutralization", "")

    region = settings.get("region", "")

    maxTrade = settings.get("maxTrade", 0)

    

    # å®‰å…¨è·å– pyramids æ•°æ®

    matches_pyramid = next((check for check in is_data.get('checks', []) if check.get('name') == 'MATCHES_PYRAMID'), None)

    pyramids = [p.get('name', '') for p in matches_pyramid.get('pyramids', [])] if matches_pyramid else []




    # æŸ¥æ‰¾ LOW_ROBUST_UNIVERSE_SHARPE

    robust_sharpe = 0.0

    robust_sharpe_check = next((check for check in is_data.get('checks', []) if check.get('name') == 'LOW_ROBUST_UNIVERSE_SHARPE'), None)

    if robust_sharpe_check:

        robust_sharpe = robust_sharpe_check.get('value', 0.0)

    

    triple = [alpha_id, sharpe, turnover, fitness, margin, exp, region, universe, neutralization, decay, delay, truncation, maxTrade, pyramids, robust_sharpe]

    return triple




def get_pnl(s, alpha_id):

    """

    Fetches the profit and loss (PnL) data for a given alpha ID by making requests to an API endpoint.

    The function handles retry logic for requests when a 'Retry-After' header

    is present in the response from the server.




    Parameters:

        s: requests.Session

            The session object used to make requests to the API.

        alpha_id: str

            The unique identifier of the alpha whose PnL data is to be fetched.




    Returns:

        requests.Response

            The API response containing PnL data.




    """

    while True:

        pnl = s.get(f'{brain_api_url}/alphas/{alpha_id}/recordsets/pnl')

        if pnl.headers.get('Retry-After', 0) == 0:

             break

        time.sleep(float(pnl.headers['Retry-After']))

    return pnl




def modify_alpha_expression(original_exp, modification_type, value):

    """

    æ ¹æ®æŒ‡å®šçš„ä¿®æ”¹ç±»å‹å’Œå€¼ï¼Œæ™ºèƒ½åœ°ä¿®æ”¹Alphaè¡¨è¾¾å¼å­—ç¬¦ä¸²ã€‚

    """

    modified_exp = original_exp




    if modification_type == "time_backfill_ts":

        # æŸ¥æ‰¾ ts_backfill(X, N) å¹¶ä¿®æ”¹ N

        # åŒ¹é… ts_backfill( ä»»æ„éé€—å·å­—ç¬¦ , ä»»æ„æ•°å­— )

        match = re.search(r"ts_backfill\(([^,]+),\s*(\d+)\)", original_exp)

        if match:

            # æ›¿æ¢æ•è·ç»„2ï¼ˆæ•°å­—ï¼‰ä¸ºæ–°çš„å€¼

            modified_exp = re.sub(r"ts_backfill\(([^,]+),\s*(\d+)\)", fr"ts_backfill(\1, {value})", original_exp, 1)

        else:

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ° ts_backfillï¼Œåˆ™å°è¯•æ·»åŠ 

            modified_exp = f"ts_backfill({original_exp}, {value})"

            

    elif modification_type == "time_backfill_group":

        # æŸ¥æ‰¾ group_backfill(X, Y, N) å¹¶ä¿®æ”¹ N

        # åŒ¹é… group_backfill( ä»»æ„éé€—å·å­—ç¬¦ , ä»»æ„éé€—å·å­—ç¬¦ , ä»»æ„æ•°å­— )

        match = re.search(r"group_backfill\(([^,]+),\s*([^,]+),\s*(\d+)\)", original_exp)

        if match:

            # æ›¿æ¢æ•è·ç»„3ï¼ˆæ•°å­—ï¼‰ä¸ºæ–°çš„å€¼

            modified_exp = re.sub(r"group_backfill\(([^,]+),\s*([^,]+),\s*(\d+)\)", fr"group_backfill(\1, \2, {value})", original_exp, 1)

        else:

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ° group_backfillï¼Œåˆ™å°è¯•æ·»åŠ 

            # å‡è®¾ group_backfill éœ€è¦ä¸€ä¸ª group å‚æ•°ï¼Œè¿™é‡Œé»˜è®¤ä½¿ç”¨ 'sector'

            modified_exp = f"group_backfill({original_exp}, sector, {value})"




    elif modification_type == "add_winsorize":

        # å°† original_exp ç”¨ winsorize(original_exp, std=value) åŒ…è£…

        modified_exp = f"winsorize({original_exp}, std={value})"




    elif modification_type == "add_signed_power":

        # å°† original_exp ç”¨ signed_power(original_exp, value) åŒ…è£…

        modified_exp = f"signed_power({original_exp}, {value})"




    elif modification_type == "add_group_zscore":

        # å°† original_exp ç”¨ group_zscore(original_exp, value) åŒ…è£…

        # value é¢„æœŸä¸º 'sector' æˆ– 'industry'

        modified_exp = f"group_zscore({original_exp}, {value})"




    elif modification_type == "winsorize_std":

        # æŸ¥æ‰¾ winsorize(X, std=N) å¹¶ä¿®æ”¹ N

        match = re.search(r"winsorize\(([^,]+),\s*std=(\d+)\)", original_exp)

        if match:

            modified_exp = re.sub(r"winsorize\(([^,]+),\s*std=(\d+)\)", fr"winsorize(\1, std={value})", original_exp, 1)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ° winsorizeï¼Œåˆ™ä¸è¿›è¡Œä¿®æ”¹ï¼Œæˆ–è€…å¯ä»¥è€ƒè™‘æ·»åŠ ï¼Œä½†è¿™é‡Œé€‰æ‹©ä¸ä¿®æ”¹

        

    else:

        print(f"æœªçŸ¥ä¿®æ”¹ç±»å‹: {modification_type}")




    return modified_exp




def simulate_multis(session_manager, alphas, name, tags):

    """

    æ¨¡æ‹Ÿå¤šä¸ªalphaè¡¨è¾¾å¼å¯¹åº”çš„æŸä¸ªåœ°åŒºçš„ä¿¡æ¯

    """

    if session_manager.session is None:

        session_manager.refresh_session()

    if time.time() - session_manager.start_time > session_manager.expiry_time:

        session_manager.refresh_session()




    result_ids = []  # ç”¨äºå­˜å‚¨alpha_idç»“æœ

    

    if len(alphas) >1:

        while True:

            try:

                resp = session_manager.session.post(f'{brain_api_url}/simulations',

                                                    json=alphas)

                simulation_progress_url = resp.headers.get('Location', 0)

                if simulation_progress_url == 0:

                    json_data = resp.json()

                    print(json_data)

                    if isinstance(json_data, list):

                        detail = json_data[0].get("detail", 0) if json_data else 0

                    else:

                        detail = json_data.get("detail", 0)

                    if 'SIMULATION_LIMIT_EXCEEDED' in detail:

                        print("Limited by the number of simulations allowed per time")

                        time.sleep(1)

                    else:

                        print("detail:", detail)

                        print("json_data:", json_data)

                        print("Alpha expression is duplicated")

                        time.sleep(1)

                        return result_ids

                else:

                    print('simulation_progress_url:', simulation_progress_url)

                    break

            except KeyError:

                print("Location key error during simulation request")

                time.sleep(60)

            except Exception as e:

                print("An error occurred1:", str(e))

                time.sleep(60)

        # æ£€æŸ¥è¿›åº¦é˜¶æ®µè¶…æ—¶æ§åˆ¶ï¼ˆ20åˆ†é’Ÿï¼‰

        get_start_time = time.time()

        while True:

            if time.time() - get_start_time > 1200:

                print(f"æ¨¡æ‹Ÿè¿›åº¦æ£€æŸ¥è¶…æ—¶(20åˆ†é’Ÿ), alpha: {alphas}, progress_url: {simulation_progress_url}")

                return result_ids

            try:

                resps = session_manager.session.get(simulation_progress_url)

                json_data = resps.json()

                # æå‰åˆå§‹åŒ–childrenå˜é‡ï¼Œç¡®ä¿æ‰€æœ‰è·¯å¾„éƒ½èƒ½è®¿é—®

                children = json_data.get("children", [])

                # è·å–å“åº”å¤´

                headers = resps.headers

                retry_after = headers.get('Retry-After', 0)

                if retry_after == 0:

                    status = json_data.get("status", 0)

                    if status == 'ERROR':

                        print(f"Error in simulation: {simulation_progress_url}")

                    elif status != "COMPLETE":

                        print(f"Simulation not complete: {simulation_progress_url}")

                        delete_resp = session_manager.session.delete(simulation_progress_url)                    

                        delete_json_data = delete_resp.json()

                        if delete_json_data.get("detail", 0) == "æœªæ‰¾åˆ°ã€‚":

                            print("Successfully deleted: %s", simulation_progress_url)

                        else:

                            print("Failed to delete: %s", simulation_progress_url)

                    else:

                        print('Simulation completed: %s', simulation_progress_url)

                    break

                time.sleep(float(retry_after))

            except Exception as e:

                print(f"Progress check error: %s", str(e))

                time.sleep(30) 

        

        # å°†forå¾ªç¯ç§»åˆ°whileå¾ªç¯ä¹‹å¤–

        for alpha, child in zip(alphas, children):

            try:

                child_str = str(child) # æ–°å¢ï¼šç¡®ä¿childæ˜¯å­—ç¬¦ä¸²

                child_progress = session_manager.session.get(f"{brain_api_url}/simulations/" + child_str)

                json_data = child_progress.json()

                alpha_id = json_data["alpha"]

                print("set_alpha_properties alpha_id: %s"%alpha_id)

                set_alpha_properties(session_manager.session,

                                    alpha_id,

                                    name="%s" % name,

                                    color=None,

                                    tags=tags)

                # ä½¿ç”¨åŸå§‹alphaæ•°æ®ç”Ÿæˆå”¯ä¸€ID

                settings_str = json.dumps(alpha['settings'], sort_keys=True)  # ä½¿ç”¨åŸå§‹é…ç½®

                regular_str = alpha['regular']  # ä½¿ç”¨åŸå§‹é…ç½®

                unique_id = f"{regular_str}|{settings_str}"

                # ç¡®ä¿optimizeç›®å½•å­˜åœ¨

                os.makedirs('optimize', exist_ok=True)

                result_file_path = f'optimize/{name}_simulated_alpha_expression.txt'

                with open(result_file_path, mode='a') as f:

                    f.write(f"{alpha_id}|{unique_id}\n")

                # å°†alpha_idæ·»åŠ åˆ°ç»“æœåˆ—è¡¨

                result_ids.append(alpha_id)

            except KeyError:

                print("Failed to retrieve alpha ID for: %s" % (f"{brain_api_url}/simulations/" + child))

                try:

                    # å…³è”åŸå§‹alphaä¿¡æ¯å¹¶è·å–é”™è¯¯çŠ¶æ€å’Œæ¶ˆæ¯

                    settings_str = json.dumps(alpha['settings'], sort_keys=True)  # ä½¿ç”¨åŸå§‹é…ç½®

                    regular_str = alpha['regular']  # ä½¿ç”¨åŸå§‹é…ç½®

                    unique_id = f"{regular_str}|{settings_str}"

                    status = json_data.get("status")

                    if status == "ERROR":

                        error_msg = json_data.get("message", "No error message available")

                        error_str = f"ERROR_{error_msg}"

                        print("write error msg to file")

                        # ç¡®ä¿optimizeç›®å½•å­˜åœ¨

                        os.makedirs('optimize', exist_ok=True)

                        result_file_path = f'optimize/{name}_simulated_alpha_expression.txt'

                        with open(result_file_path, mode='a') as f:

                            f.write(f"{error_str}|{unique_id}\n")

                except Exception as e:

                    print("get error status :",str(e)) 

            except Exception as e:

                print("An error occurred while setting alpha properties:" + str(e))

        return result_ids  # å°†returnç§»è‡³forå¾ªç¯å¤–éƒ¨

    else:

        result_ids = []

        simulation_data = alphas[0]

        while True:

            try:

                resp = session_manager.session.post(f'{brain_api_url}/simulations',

                                                    json=simulation_data)

                simulation_progress_url = resp.headers.get('Location', 0)

                if simulation_progress_url == 0:

                    json_data = resp.json()

                    if isinstance(json_data, list):

                        print(json_data)

                        detail = json_data[0].get("detail", 0) if json_data else 0

                    else:

                        detail = json_data.get("detail", 0)

                    if 'SIMULATION_LIMIT_EXCEEDED' in detail:

                        print("Limited by the number of simulations allowed per time")

                        time.sleep(1)

                    else:

                        print("detail:", detail)

                        print("json_data:", json_data)

                        print("Alpha expression is duplicated")

                        time.sleep(1)

                        return result_ids

                else:

                    print('simulation_progress_url:', simulation_progress_url)

                    break

            except KeyError:

                print("Location key error during simulation request")

                time.sleep(60)

            except Exception as e:

                print("An error occurred2:", str(e))

                time.sleep(60)




        # æ£€æŸ¥è¿›åº¦é˜¶æ®µè¶…æ—¶æ§åˆ¶ï¼ˆ20åˆ†é’Ÿï¼‰

        get_start_time = time.time()

        while True:

            if time.time() - get_start_time > 1200:

                print(f"æ¨¡æ‹Ÿè¿›åº¦æ£€æŸ¥è¶…æ—¶ï¼ˆ20åˆ†é’Ÿï¼‰ï¼Œalpha: {simulation_data}, progress_url: {simulation_progress_url}")

                return result_ids

            try:

                resp = session_manager.session.get(simulation_progress_url)

                json_data = resp.json()

                # è·å–å“åº”å¤´

                headers = resp.headers

                retry_after = headers.get('Retry-After', 0)

                if retry_after == 0:

                    print("response done: %s" % json_data)

                    break

                time.sleep(float(retry_after))

            except Exception as e:

                print("Error while checking progress:", str(e))

                time.sleep(60)




        print("%s done simulating, getting alpha details" % (simulation_progress_url))

        try:

            alpha_id = json_data.get("alpha")

            alpha = json_data.get("regular")

            print("set_alpha_properties alpha_id: %s"%alpha_id)

            # å‡è®¾ async_set_alpha_properties æœ‰å¯¹åº”çš„åŒæ­¥ç‰ˆæœ¬

            set_alpha_properties(session_manager.session,

                                alpha_id,

                                name="%s" % name,

                                color=None,

                                tags=tags)




            

            settings_str = json.dumps(simulation_data['settings'], sort_keys=True)  # æ”¹ä¸ºä½¿ç”¨åŸå§‹é…ç½®

            regular_str = simulation_data['regular']  # æ”¹ä¸ºä½¿ç”¨åŸå§‹é…ç½®

            unique_id = f"{regular_str}|{settings_str}"

            # ç¡®ä¿optimizeç›®å½•å­˜åœ¨

            os.makedirs('optimize', exist_ok=True)

            result_file_path = f'optimize/{name}_simulated_alpha_expression.txt'

            with open(result_file_path, mode='a') as f:

                f.write(f"{alpha_id}|{unique_id}\n")

            result_ids.append(alpha_id)




        except KeyError:

            print("Failed to retrieve alpha ID for: %s" % simulation_progress_url)

            try:

                # å…³è”åŸå§‹alphaä¿¡æ¯å¹¶è·å–é”™è¯¯çŠ¶æ€å’Œæ¶ˆæ¯

                settings_str = json.dumps(simulation_data['settings'], sort_keys=True)  # ä½¿ç”¨åŸå§‹é…ç½®

                regular_str = simulation_data['regular']  # ä½¿ç”¨åŸå§‹é…ç½®

                unique_id = f"{regular_str}|{settings_str}"

                status = json_data.get("status")

                if status == "ERROR":

                    error_msg = json_data.get("message", "No error message available")

                    error_str = f"ERROR_{error_msg}"

                    print("write error msg to file")

                    # ç¡®ä¿optimizeç›®å½•å­˜åœ¨

                    os.makedirs('optimize', exist_ok=True)

                    result_file_path = f'optimize/{name}_simulated_alpha_expression.txt'

                    with open(result_file_path, mode='a') as f:

                        f.write(f"{error_str}|{unique_id}\n")

            except Exception as e:

                print("get error status :",str(e))

        except Exception as e:

            print("An error occurred while setting alpha properties:", str(e))




        return result_ids  # è¿”å›æ”¶é›†çš„alpha_idåˆ—è¡¨ï¼Œæ¯ä¸ªIDå‡ºç°ä¸¤æ¬¡




def simulate_multiple_alphas_with_retry(alpha_list, name="optimize_alpha", n_jobs=8, max_retries=5, is_neut=False):

    """

    åŒ…è£…simulate_multiple_alphaså‡½æ•°ï¼Œä½¿ç”¨é˜Ÿåˆ—æ–¹å¼æä¾›è‡ªåŠ¨é‡è¯•åŠŸèƒ½

    å½“ç»“æœåˆ—è¡¨é•¿åº¦ç­‰äºåˆå§‹alpha_listé•¿åº¦æˆ–é‡è¯•æ¬¡æ•°è¾¾åˆ°ä¸Šé™æ—¶é€€å‡º

    """

    original_alpha_count = len(alpha_list)

    all_results = []

    retries = 0

    # åˆ›å»ºç”¨äºå­˜å‚¨ç»“æœçš„æ–‡ä»¶è·¯å¾„

    result_file_path = f'optimize/{name}_simulated_alpha_expression.txt'

    os.makedirs('optimize', exist_ok=True)

    

    while retries < max_retries:

        # ä»æ–‡ä»¶ä¸­è¯»å–å·²å®Œæˆçš„alphaè¡¨è¾¾å¼

        completed_alphas = set()

        try:

            with open(result_file_path, mode='r') as f:

                for line in f:

                    completed_alphas.add(line.strip())

            print(f"ä»æ–‡ä»¶ä¸­è¯»å–åˆ°{len(completed_alphas)}ä¸ªå·²å®Œæˆçš„alpha")

        except FileNotFoundError:

            print(f"æ–‡ä»¶{result_file_path}ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶")

        

        # è¿‡æ»¤å‡ºå°šæœªå®Œæˆçš„alpha

        remaining_alphas = []

        for alpha in alpha_list:

            # ç”Ÿæˆå”¯ä¸€æ ‡è¯†

            settings_str = json.dumps(alpha['settings'], sort_keys=True)

            regular_str = alpha['regular']

            unique_id = f"{regular_str}|{settings_str}"

            

            # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ

            if not any(unique_id in line for line in completed_alphas):

                remaining_alphas.append((alpha, unique_id))

            

            # æ”¶é›†å·²å®Œæˆçš„ç»“æœ

            for line in completed_alphas:

                if "|" in line and unique_id in line:

                    alpha_id = line.split("|")[0]

                    if "ERROR" not in alpha_id and alpha_id not in all_results:

                        all_results.append(alpha_id)

        

        # å¦‚æœæ‰€æœ‰alphaéƒ½å·²å®Œæˆï¼Œæå‰é€€å‡º

        if len(remaining_alphas)==0:

            print(f"æ‰€æœ‰{original_alpha_count}ä¸ªalphaå·²å®Œæˆï¼Œæ— éœ€ç»§ç»­é‡è¯•")

            return all_results

        

        # å¦‚æœè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé€€å‡º

        if retries >= max_retries:

            print(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}ï¼Œåœæ­¢é‡è¯•")

            break

        

        # å¦‚æœæ‰€æœ‰alphaéƒ½å·²å®Œæˆï¼Œæå‰é€€å‡º

        if len(remaining_alphas) == 0:

            print(f"æ‰€æœ‰{original_alpha_count}ä¸ªalphaå·²å®Œæˆï¼Œæ— éœ€ç»§ç»­é‡è¯•")

            return all_results

        

        # å¦‚æœè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé€€å‡º

        if retries >= max_retries:

            print(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}ï¼Œåœæ­¢é‡è¯•")

            break

        

        print(f"ç¬¬ {retries + 1} æ¬¡å°è¯•ï¼Œå¼€å§‹å¤„ç†{len(remaining_alphas)}/{original_alpha_count}ä¸ªæœªå®Œæˆçš„alpha")

        

        # åˆ›å»ºçº¿ç¨‹å®‰å…¨çš„ä»»åŠ¡é˜Ÿåˆ—

        task_queue = queue.Queue()

        # å°†æ‰€æœ‰ä»»åŠ¡æ·»åŠ åˆ°é˜Ÿåˆ—

        for alpha, unique_id in remaining_alphas:

            task_queue.put((alpha, unique_id))

        

        # ç™»å½•å¹¶åˆ›å»ºä¼šè¯ç®¡ç†å™¨

        session = login()

        session_start_time = time.time()

        session_expiry_time = 3 * 60 * 60  # 3å°æ—¶

        session_manager = SessionManager(session, session_start_time, session_expiry_time)

        

        BATCH_SIZE = min(8, len(remaining_alphas)) - retries

        BATCH_SIZE = max(1, BATCH_SIZE) # ç¡®ä¿ BATCH_SIZE è‡³å°‘ä¸º 1

        

        alpha_for_region_check = remaining_alphas[0][0] # ç¡®ä¿ remaining_alphas å·²ç»æ£€æŸ¥ä¸ä¸ºç©º

        if alpha_for_region_check.get('settings').get('region') == "GLB":

            BATCH_SIZE = min(6, len(remaining_alphas)) - retries

            BATCH_SIZE = max(1, BATCH_SIZE) # ç¡®ä¿ GLB åŒºåŸŸçš„ BATCH_SIZE ä¹Ÿè‡³å°‘ä¸º 1

            if len(remaining_alphas) < 10 and BATCH_SIZE > 3:

                BATCH_SIZE = 3

        if is_neut:

            BATCH_SIZE = 1

        # æ·»åŠ æ‰¹æ¬¡è®¡æ•°å™¨

        total_batches = (len(remaining_alphas) + BATCH_SIZE - 1) // BATCH_SIZE

        completed_batches = 0

        processed_tasks = 0 

        batch_lock = threading.Lock()  # ç”¨äºä¿æŠ¤æ‰¹æ¬¡è®¡æ•°å™¨çš„é”




        # å·¥ä½œçº¿ç¨‹å‡½æ•°

        def worker(worker_id):

            nonlocal completed_batches, total_batches, processed_tasks

            while not task_queue.empty():

                try:

                    batch = []

                    for _ in range(BATCH_SIZE):

                        try:

                            item = task_queue.get(timeout=1)

                            batch.append(item)

                        except queue.Empty:

                            break

        

                    if not batch:

                        break  # é˜Ÿåˆ—ä¸ºç©ºï¼Œé€€å‡ºå¾ªç¯

        

                    # å¤„ç†æ‰¹æ¬¡ä»»åŠ¡

                    alphas_to_simulate = [item[0] for item in batch]

                    unique_ids_for_batch = [item[1] for item in batch]

            

                    # è°ƒç”¨æ¨¡æ‹Ÿå‡½æ•°

                    result_ids = simulate_multis(session_manager, alphas_to_simulate, name, [name])

            

                    # è®°å½•ç»“æœ

                    if result_ids:

                        for i, alpha_id in enumerate(result_ids):

                            if i < len(unique_ids_for_batch):

                                unique_id = unique_ids_for_batch[i]

                                if alpha_id and "ERROR" not in alpha_id:

                                    with open(result_file_path, mode='a') as f:

                                        f.write(f"{alpha_id}|{unique_id}\n")

                                    if alpha_id not in all_results:

                                        all_results.append(alpha_id)

        

                    # æ›´æ–°æ‰¹æ¬¡è®¡æ•°å™¨å¹¶æ‰“å°è¿›åº¦

                    with batch_lock:

                        completed_batches += 1

                        processed_tasks += len(batch)

                        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

                        print(f"[{current_time}] å®Œæˆæ‰¹æ¬¡ {completed_batches}/{total_batches}ï¼Œç´¯è®¡å¤„ç†äº† {processed_tasks}/{len(remaining_alphas)} ä¸ªä»»åŠ¡")

                except Exception as e:

                    print(f"æ‰¹æ¬¡ä»»åŠ¡å¤±è´¥ï¼šé”™è¯¯={type(e).__name__}-{str(e)}")

                finally:

                    # æ ‡è®°æ‰¹æ¬¡ä¸­æ‰€æœ‰ä»»åŠ¡ä¸ºå®Œæˆ

                    if 'batch' in locals():

                        for _ in batch:

                            task_queue.task_done()

        

        # åˆ›å»ºçº¿ç¨‹æ± 

        with concurrent.futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:

            # æäº¤å·¥ä½œçº¿ç¨‹

            futures = [executor.submit(worker, i) for i in range(n_jobs)]

        

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ

            task_queue.join()

        

            # æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„future

            for future in concurrent.futures.as_completed(futures):

                try:

                    future.result()

                except Exception as e:

                    print(f"çº¿ç¨‹æ‰§è¡Œå¼‚å¸¸: {str(e)}")

        

        # å…³é—­ä¼šè¯

        try:

            if session_manager.session:

                session_manager.session.close()

        except Exception as e:

            print(f"å…³é—­ä¼šè¯å¤±è´¥: {str(e)}")

        

        retries += 1

        

        print(f"ç¬¬ {retries} æ¬¡å°è¯•å®Œæˆï¼Œå½“å‰æˆåŠŸè·å–{len(all_results)}/{original_alpha_count}ä¸ªalphaç»“æœ")

    

    print(f"å®Œæˆå¤„ç†ï¼ŒæˆåŠŸè·å–{len(all_results)}/{original_alpha_count}ä¸ªalphaç»“æœ")

    return all_results




def runRobustSharpe(s, details):

    # 1. è·å–åŸå§‹Alphaä¿¡æ¯

    # [alpha_id, sharpe, turnover, fitness, margin, exp, region, universe, neutralization, decay, delay, truncation, maxTrade, pyramids, robust_sharpe]

    original_alpha_id, original_sharpe, _, _, _, original_exp, region, universe, original_neutralization, original_decay, delay, original_truncation, maxTrade, _, original_robust_sharpe = details




    print(f"ğŸš€ å¼€å§‹Robust Sharpeä¼˜åŒ–: {original_alpha_id} - {original_exp}")

    print(f"ğŸ“Š åŸå§‹é…ç½®: region={region}, universe={universe}, delay={delay}, neutralization={original_neutralization}, decay={original_decay}, truncation={original_truncation}, robust_sharpe={original_robust_sharpe:.2f}, sharpe={original_sharpe:.2f}")




    # å­˜å‚¨æ‰€æœ‰ä¸­é—´ç»“æœï¼Œæ–¹ä¾¿è°ƒè¯•å’Œæœ€ç»ˆç­›é€‰

    all_results = []




    # --- é˜¶æ®µ1: ä¸­æ€§åŒ–æ–¹æ³•éå†ä¸åˆæ­¥ç­›é€‰ ---

    print("\n--- é˜¶æ®µ1: ä¸­æ€§åŒ–æ–¹æ³•éå† ---")

    neutralizations = NEUT_DICTS[region] # è·å–è¯¥åœ°åŒºæ”¯æŒçš„ä¸­æ€§åŒ–åˆ—è¡¨

    neut_alpha_configs = []

    for neut in neutralizations:

        config = {

            'type': 'REGULAR',

            'settings': {

                'instrumentType': 'EQUITY',

                'region': region,

                'universe': universe,

                'delay': delay,

                'decay': original_decay,

                'neutralization': neut,

                'truncation': original_truncation,

                'pasteurization': 'ON',

                'unitHandling': 'VERIFY',

                'nanHandling': 'ON',

                'language': 'FASTEXPR',

                'visualization': False,

                'testPeriod': "P0Y",

                'maxTrade': maxTrade

            },

            'regular': original_exp

        }

        neut_alpha_configs.append(config)




    print(f"ğŸ‘¨â€ğŸ’» ç”Ÿæˆäº† {len(neut_alpha_configs)} ä¸ªä¸­æ€§åŒ–é…ç½®è¿›è¡Œæ¨¡æ‹Ÿ.")

    neut_result_ids = simulate_multiple_alphas_with_retry(neut_alpha_configs, name=f"robust_sharpe_optimized")




    detailed_neut_results = []

    for alpha_id in neut_result_ids:

        if alpha_id == "None":

            continue

        # [alpha_id, sharpe, turnover, fitness, margin, exp, region, universe, neutralization, decay, delay, truncation, maxTrade, pyramids, robust_sharpe]

        current_details = locate_details(s, alpha_id)

        current_sharpe = current_details[1]

        current_robust_sharpe = current_details[-1]

        current_neutralization = current_details[8]




        if current_sharpe > 1.2: # åˆæ­¥ç­›é€‰ï¼šalpha sharpe > 1.2

            detailed_neut_results.append({

                'alpha_id': alpha_id,

                'sharpe': current_sharpe,

                'robust_sharpe': current_robust_sharpe,

                'neutralization': current_neutralization,

                'decay': original_decay,

                'truncation': original_truncation,

                'exp': original_exp # è®°å½•å½“å‰ä½¿ç”¨çš„è¡¨è¾¾å¼

            })

    

    # ä¸¥æ ¼é€‰æ‹©å‰ä¸¤ä¸ªæœ€ä½³ä¸­æ€§åŒ–é…ç½®

    detailed_neut_results.sort(key=lambda x: x['robust_sharpe'], reverse=True)

    best_neut_configs = detailed_neut_results[:2]

    print(f"âœ… ç­›é€‰å‡º {len(best_neut_configs)} ä¸ªæœ€ä½³ä¸­æ€§åŒ–é…ç½®.")

    for cfg in best_neut_configs:

        print(f"   - Neut: {cfg['neutralization']}, Robust Sharpe: {cfg['robust_sharpe']:.2f}, Sharpe: {cfg['sharpe']:.2f}")




    # --- é˜¶æ®µ2: Decay/Truncationå‚æ•°éå†ä¸è¿›ä¸€æ­¥ç­›é€‰ ---

    print("\n--- é˜¶æ®µ2: Decay/Truncationå‚æ•°éå† ---")

    best_base_configs = [] # å­˜å‚¨æœ€ç»ˆé€‰å‡ºçš„æœ€ä½³ä¸­æ€§åŒ–ã€decayã€truncationç»„åˆ




    decay_options = [original_decay, 10, 30, 60] # ç¤ºä¾‹å€¼ï¼Œå¯è°ƒæ•´

    truncation_options = [original_truncation, 0.01, 0.03, 0.05] # ç¤ºä¾‹å€¼ï¼Œå¯è°ƒæ•´




    for neut_cfg in best_neut_configs:

        current_neutralization = neut_cfg['neutralization']

        decay_trunc_alpha_configs = []

        for decay_val in decay_options:

            for trunc_val in truncation_options:

                config = {

                    'type': 'REGULAR',

                    'settings': {

                        'instrumentType': 'EQUITY',

                        'region': region,

                        'universe': universe,

                        'delay': delay,

                        'decay': decay_val,

                        'neutralization': current_neutralization,

                        'truncation': trunc_val,

                        'pasteurization': 'ON',

                        'unitHandling': 'VERIFY',

                        'nanHandling': 'ON',

                        'language': 'FASTEXPR',

                        'visualization': False,

                        'testPeriod': "P0Y",

                        'maxTrade': maxTrade

                    },

                    'regular': original_exp

                }

                decay_trunc_alpha_configs.append(config)

        

        print(f"ğŸ‘¨â€ğŸ’» ä¸ºä¸­æ€§åŒ– {current_neutralization} ç”Ÿæˆäº† {len(decay_trunc_alpha_configs)} ä¸ªDecay/Truncationé…ç½®è¿›è¡Œæ¨¡æ‹Ÿ.")

        decay_trunc_result_ids = simulate_multiple_alphas_with_retry(decay_trunc_alpha_configs, name=f"robust_sharpe_optimized")




        detailed_decay_trunc_results = []

        for alpha_id in decay_trunc_result_ids:

            if alpha_id == "None":

                continue

            current_details = locate_details(s, alpha_id)

            current_sharpe = current_details[1]

            current_robust_sharpe = current_details[-1]

            current_decay = current_details[9]

            current_truncation = current_details[11]




            if current_sharpe > 1.2: # è¿›ä¸€æ­¥ç­›é€‰ï¼šalpha sharpe > 1.2

                detailed_decay_trunc_results.append({

                    'alpha_id': alpha_id,

                    'sharpe': current_sharpe,

                    'robust_sharpe': current_robust_sharpe,

                    'neutralization': current_neutralization,

                    'decay': current_decay,

                    'truncation': current_truncation,

                    'exp': original_exp

                })

        

        # ä¸¥æ ¼é€‰æ‹©å‰ä¸¤ä¸ªæœ€ä½³Decay/Truncationç»„åˆ

        detailed_decay_trunc_results.sort(key=lambda x: x['robust_sharpe'], reverse=True)

        best_base_configs.extend(detailed_decay_trunc_results[:2])

    

    print(f"âœ… ç­›é€‰å‡º {len(best_base_configs)} ä¸ªæœ€ä½³åŸºç¡€é…ç½® (ä¸­æ€§åŒ–+Decay+Truncation).")

    for cfg in best_base_configs:

        print(f"   - Neut: {cfg['neutralization']}, Decay: {cfg['decay']:.2f}, Trunc: {cfg['truncation']:.2f}, Robust Sharpe: {cfg['robust_sharpe']:.2f}, Sharpe: {cfg['sharpe']:.2f}")




    # --- é˜¶æ®µ3: ç”Ÿæˆä¼˜åŒ–åçš„Alphaè¡¨è¾¾å¼å˜ä½“ ---

    print("\n--- é˜¶æ®µ3: ç”Ÿæˆä¼˜åŒ–åçš„Alphaè¡¨è¾¾å¼å˜ä½“ ---")

    optimized_alpha_variants = []




    # å®šä¹‰è¡¨è¾¾å¼ä¿®æ”¹çš„é€‰é¡¹

    expression_modifications = [

        ("time_backfill_ts", 75), ("time_backfill_ts", 90),

        ("time_backfill_group", 180), ("time_backfill_group", 275),

        ("add_winsorize", 3),

        ("add_signed_power", 0.5),("add_signed_power", 1.5),("add_signed_power", 2),

        ("add_group_zscore", "sector"), # Assuming 'sector' as default group for zscore

        ("winsorize_std", 3), ("winsorize_std", 5) # Assuming original_std for winsorize was 4, offering alternatives

    ]




    for base_cfg in best_base_configs:

        current_exp = base_cfg['exp']

        current_neutralization = base_cfg['neutralization']

        current_decay = base_cfg['decay']

        current_truncation = base_cfg['truncation']




        # åŸå§‹è¡¨è¾¾å¼ä½œä¸ºåŸºå‡†å˜ä½“

        optimized_alpha_variants.append({

            'type': 'REGULAR',

            'settings': {

                'instrumentType': 'EQUITY',

                'region': region,

                'universe': universe,

                'delay': delay,

                'decay': current_decay,

                'neutralization': current_neutralization,

                'truncation': current_truncation,

                'pasteurization': 'ON',

                'unitHandling': 'VERIFY',

                'nanHandling': 'ON',

                'language': 'FASTEXPR',

                'visualization': False,

                'testPeriod': "P0Y",

                'maxTrade': maxTrade

            },

            'regular': current_exp

        })




        for mod_type, mod_val in expression_modifications:

            modified_exp = modify_alpha_expression(current_exp, mod_type, mod_val)

            if modified_exp != current_exp: # ç¡®ä¿è¡¨è¾¾å¼ç¡®å®è¢«ä¿®æ”¹äº†

                optimized_alpha_variants.append({

                    'type': 'REGULAR',

                    'settings': {

                        'instrumentType': 'EQUITY',

                        'region': region,

                        'universe': universe,

                        'delay': delay,

                        'decay': current_decay,

                        'neutralization': current_neutralization,

                        'truncation': current_truncation,

                        'pasteurization': 'ON',

                        'unitHandling': 'VERIFY',

                        'nanHandling': 'ON',

                        'language': 'FASTEXPR',

                        'visualization': False,

                        'testPeriod': "P0Y",

                        'maxTrade': maxTrade

                    },

                    'regular': modified_exp

                })

    

    print(f"ğŸ‘¨â€ğŸ’» ç”Ÿæˆäº† {len(optimized_alpha_variants)} ä¸ªä¼˜åŒ–åçš„Alphaè¡¨è¾¾å¼å˜ä½“è¿›è¡Œæ¨¡æ‹Ÿ.")

    optimized_result_ids = simulate_multiple_alphas_with_retry(optimized_alpha_variants, name=f"robust_sharpe_optimized")




    # --- é˜¶æ®µ4: éªŒè¯ä¸ç»“æœè¿”å› ---

    print("\n--- é˜¶æ®µ4: éªŒè¯ä¸ç»“æœè¿”å› ---")

    all_final_stage_alphas = [] # New list to store all results

    satisfied_count = 0




    for alpha_id in optimized_result_ids:

        if alpha_id == "None":

            continue

        

        current_details = locate_details(s, alpha_id)

        current_sharpe = current_details[1]

        current_robust_sharpe = current_details[-1]

        current_exp = current_details[5]

        current_neutralization = current_details[8]

        current_decay = current_details[9]

        current_truncation = current_details[11]




        # Get basecheck result (Pass/Fail)

        alpha_detail = get_alpha_byid(s, alpha_id)

        result_basecheck = 'Pass'

        if alpha_detail:

            checks = alpha_detail['is']['checks']

            # Check if any basic checks fail or error

            if any(check.get("result") == "FAIL" or check.get("result") == "ERROR" for check in checks):

                result_basecheck = 'Fail'

            # Additional check for "Weight is too strongly"

            if "Weight is too strongly" in str(checks):

                result_basecheck = 'Fail'




        is_satisfied = (current_robust_sharpe >= 1.0 and current_sharpe > 1.2)

        if is_satisfied:

            satisfied_count += 1




        all_final_stage_alphas.append({

            'alpha_id': alpha_id,

            'optimized_expression': current_exp,

            'neutralization': current_neutralization,

            'decay': current_decay,

            'truncation': current_truncation,

            'robust_sharpe': current_robust_sharpe,

            'sharpe': current_sharpe,

            'basecheck_result': result_basecheck, # Add basecheck result

            'is_satisfied': is_satisfied # Add satisfaction flag

        })

    

    print(f"ğŸ‰ æˆåŠŸä¼˜åŒ–å‡º {satisfied_count} ä¸ªAlphaæ»¡è¶³æ¡ä»¶.")

    print(f"æ€»å…±å¤„ç†äº† {len(all_final_stage_alphas)} ä¸ªæœ€ç»ˆé˜¶æ®µçš„Alphaã€‚")




    if all_final_stage_alphas:

        # Print details for satisfied alphas

        print("\n--- æ»¡è¶³æ¡ä»¶çš„ä¼˜åŒ–Alphaè¯¦æƒ… ---")

        for alpha in all_final_stage_alphas:

            if alpha['is_satisfied']:

                print(f"   - Alpha ID: {alpha['alpha_id']}, Robust Sharpe: {alpha['robust_sharpe']:.2f}, Sharpe: {alpha['sharpe']:.2f}")

                print(f"     Expression: {alpha['optimized_expression']}")

                print(f"     Settings: Neut={alpha['neutralization']}, Decay={alpha['decay']:.2f}, Trunc={alpha['truncation']:.2f}")

                print(f"     Basecheck: {alpha['basecheck_result']}")

        

        # Save all results to CSV

        df_results = pd.DataFrame(all_final_stage_alphas)

        save_path = os.path.join("optimize", f"{original_alpha_id}_robust_sharpe_all_results.csv")

        df_results.to_csv(save_path, index=False)

        print(f"\næ‰€æœ‰æœ€ç»ˆé˜¶æ®µçš„ä¼˜åŒ–ç»“æœå·²ä¿å­˜è‡³ï¼š{save_path}")

    else:

        print("æœªèƒ½æ‰¾åˆ°ä»»ä½•æœ€ç»ˆé˜¶æ®µçš„ä¼˜åŒ–Alphaã€‚")




    return all_final_stage_alphas # Return all results




def main():

    parser = argparse.ArgumentParser(description='Optimize Alpha expressions for Robust Sharpe.')

    parser.add_argument('alpha_id', help='The Alpha ID to optimize.')

    

    args = parser.parse_args()

    

    s = login()

    details = locate_details(s, args.alpha_id)

    

    runRobustSharpe(s, details)




if __name__ == '__main__':

    main()
