#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
WorldQuant Alphaæ•°æ®çˆ¬è™« - ç»Ÿä¸€è„šæœ¬
æ•´åˆAPIå®¢æˆ·ç«¯åŠŸèƒ½ï¼Œå®ç°ä¸€é¡µä¸€å…¥åº“çš„åŠŸèƒ½
"""

import os
import sys
import json
import time
import logging
import base64
import random
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import requests
import mysql.connector
from mysql.connector import Error
from urllib.parse import urlencode

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å°è¯•å¯¼å…¥å…¬å…±sessionç®¡ç†å™¨ï¼ˆå¯é€‰ï¼Œä¸å½±å“ç°æœ‰åŠŸèƒ½ï¼‰
try:
    from common.session_manager import get_shared_session
    SHARED_SESSION_AVAILABLE = True
except ImportError:
    SHARED_SESSION_AVAILABLE = False

# é…ç½®æ—¥å¿—
# ç¡®ä¿logç›®å½•å­˜åœ¨
log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'log')
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(log_dir, 'alpha_crawler.log'), encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AlphaCrawler:
    """Alphaæ•°æ®çˆ¬è™« - ç»Ÿä¸€è„šæœ¬"""
    
    def __init__(self, use_shared_session=True):
        """
        åˆå§‹åŒ–Alphaçˆ¬è™«
        
        Args:
            use_shared_session: æ˜¯å¦ä½¿ç”¨å…¬å…±sessionï¼ˆé»˜è®¤Trueï¼‰
        """
        # åŠ è½½é…ç½®
        self.config = self.load_config()
        self.use_shared_session = use_shared_session
        
        # APIé…ç½® - æ•´åˆAPIå®¢æˆ·ç«¯åŠŸèƒ½
        self.base_url = "https://api.worldquantbrain.com"
        
        # ã€é‡è¦ã€‘å…ˆåˆ›å»ºä¸€ä¸ªåŸºç¡€sessionï¼Œç¡®ä¿self.sessionå§‹ç»ˆå¯ç”¨
        self.session = requests.Session()
        self.session.timeout = 60
        
        self.token = None
        self.is_authenticated = False
        self.auth_time = None  # è®¤è¯æ—¶é—´è®°å½•
        
        # åŸºç¡€è¿‡æ»¤æ¡ä»¶ - ç»Ÿä¸€çš„ç”Ÿäº§ç‰ˆæœ¬
        self.base_filters = {
            
            # 'status': 'UNSUBMITTED%1FIS_FAIL',
            # 'color': 'GREEN',
            # 'settings.region': 'EUR'
        }
        
        # æ•°æ®åº“è¿æ¥
        self.db_connection = None
        
        # è®¾ç½®æµè§ˆå™¨å¤´ï¼ˆå¿…é¡»åœ¨sessionåˆ›å»ºåï¼‰
        self.setup_browser_headers()
        
        # ã€å¯é€‰ã€‘å°è¯•ä½¿ç”¨å…¬å…±sessionï¼ˆå¦‚æœå¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨ä¸Šé¢åˆ›å»ºçš„sessionï¼‰
        self._try_shared_session()
    
    def load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config', 'credentials.json')
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info("é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
            return config
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return {}
    
    def _try_shared_session(self):
        """
        å°è¯•ä½¿ç”¨å…¬å…±sessionï¼ˆå¯é€‰ä¼˜åŒ–ï¼Œä¸å½±å“åŸæœ‰åŠŸèƒ½ï¼‰
        
        ç‰¹ç‚¹ï¼š
        - å¦‚æœå…¬å…±sessionå¯ç”¨ï¼Œæ›¿æ¢å½“å‰sessionï¼ˆå·²è®¤è¯ï¼‰
        - å¦‚æœä¸å¯ç”¨ï¼Œä¿æŒä½¿ç”¨__init__ä¸­åˆ›å»ºçš„sessionï¼ˆéœ€è¦åç»­è®¤è¯ï¼‰
        - å®Œå…¨å‘åå…¼å®¹ï¼Œé›¶ç ´åæ€§
        """
        if not self.use_shared_session or not SHARED_SESSION_AVAILABLE:
            logger.info("ğŸ”§ ä½¿ç”¨ä¼ ç»Ÿsessionï¼ˆå…¬å…±sessionå·²ç¦ç”¨æˆ–ä¸å¯ç”¨ï¼‰")
            return
        
        try:
            shared_session = get_shared_session()
            if shared_session:
                logger.info("âœ… åˆ‡æ¢åˆ°å…¬å…±sessionï¼ˆå·²è®¤è¯ï¼Œå¯ç›´æ¥ä½¿ç”¨ï¼‰")
                self.session = shared_session
                self.is_authenticated = True
                self.auth_time = time.time()
            else:
                logger.info("ğŸ”§ ä½¿ç”¨ä¼ ç»Ÿsessionï¼ˆå…¬å…±sessionè·å–å¤±è´¥ï¼‰")
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–å…¬å…±sessionå¼‚å¸¸: {e}ï¼Œç»§ç»­ä½¿ç”¨ä¼ ç»Ÿsession")
    
    def authenticate(self, force_renew: bool = False) -> bool:
        """
        APIè®¤è¯ - åŸºäºå‚è€ƒæ–‡ä»¶çš„è®¤è¯æœºåˆ¶ï¼ˆå®Œå…¨å‘åå…¼å®¹ï¼‰
        
        Args:
            force_renew: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®¤è¯
        
        Returns:
            æ˜¯å¦è®¤è¯æˆåŠŸ
        """
        try:
            # ã€å¯é€‰ã€‘å¦‚æœä½¿ç”¨å…¬å…±sessionä¸”æœªå¼ºåˆ¶åˆ·æ–°ï¼Œå°è¯•å¤ç”¨
            if self.use_shared_session and SHARED_SESSION_AVAILABLE and not force_renew:
                if self.is_authenticated:
                    logger.info("â™»ï¸  ä½¿ç”¨å…¬å…±sessionï¼Œæ— éœ€é‡æ–°è®¤è¯")
                    return True
                # å°è¯•è·å–å…¬å…±session
                try:
                    session = get_shared_session()
                    if session:
                        self.session = session
                        self.is_authenticated = True
                        self.auth_time = time.time()
                        logger.info("âœ… è·å–å…¬å…±sessionæˆåŠŸ")
                        return True
                except Exception as e:
                    logger.warning(f"âš ï¸ è·å–å…¬å…±sessionå¤±è´¥: {e}ï¼Œç»§ç»­ä½¿ç”¨ä¼ ç»Ÿè®¤è¯")
            
            # ã€ä¼ ç»Ÿè®¤è¯é€»è¾‘ - å®Œå…¨ä¿ç•™ã€‘
            if not self.config:
                logger.error("é…ç½®ä¿¡æ¯æœªåŠ è½½")
                return False
            
            email = self.config.get('email')
            password = self.config.get('password')
            
            if not email or not password:
                logger.error("é‚®ç®±æˆ–å¯†ç æœªé…ç½®")
                return False
            
            # å¼ºåˆ¶é‡æ–°è®¤è¯æ—¶åˆ›å»ºå…¨æ–°çš„session
            if force_renew:
                logger.info("å¼ºåˆ¶é‡æ–°è®¤è¯ï¼Œåˆ›å»ºå…¨æ–°session...")
                self.session = requests.Session()
                self.session.timeout = 60
                self.setup_browser_headers()
                self.auth_time = None
            
            # æ£€æŸ¥è®¤è¯æ˜¯å¦è¿‡æœŸï¼ˆ3å°æ—¶è¿‡æœŸï¼‰
            current_time = time.time()
            if not force_renew and self.auth_time and (current_time - self.auth_time) < 3 * 3600 - 300:  # æå‰5åˆ†é’Ÿé‡æ–°è®¤è¯
                logger.info("è®¤è¯ä»åœ¨æœ‰æ•ˆæœŸå†…ï¼Œæ— éœ€é‡æ–°è®¤è¯")
                return True
            
            # åˆ›å»ºBasicè®¤è¯å¤´
            credentials = f"{email}:{password}"
            encoded_credentials = base64.b64encode(credentials.encode()).decode()
            
            headers = {
                'Authorization': f'Basic {encoded_credentials}'
            }
            
            # æ·»åŠ è®¤è¯é‡è¯•æœºåˆ¶ï¼Œé¿å…429é”™è¯¯
            max_auth_retries = 50
            auth_retry_delay = 30  # ç§’
            
            for auth_attempt in range(max_auth_retries):
                try:
                    logger.info(f"å°è¯•è®¤è¯ (å°è¯• {auth_attempt + 1}/{max_auth_retries})")
                    response = self.session.post('https://api.worldquantbrain.com/authentication', headers=headers)
                    
                    if response.status_code == 201:
                        logger.info("è®¤è¯æˆåŠŸ")
                        self.auth_time = current_time  # è®°å½•è®¤è¯æ—¶é—´
                        
                        # æ£€æŸ¥è®¤è¯å“åº”ä¸­æ˜¯å¦åŒ…å«Cookie
                        if 'set-cookie' in response.headers:
                            logger.info("è®¤è¯å“åº”åŒ…å«Cookieä¿¡æ¯")
                        
                        # æ‰“å°è®¤è¯åçš„Cookieä¿¡æ¯ç”¨äºè°ƒè¯•
                        if self.session.cookies:
                            logger.info("è®¤è¯åçš„Cookieä¿¡æ¯:")
                            for cookie in self.session.cookies:
                                logger.info(f"  {cookie.name}: {cookie.value[:50]}...")
                        
                        # è®¤è¯æˆåŠŸï¼Œsessionå·²è‡ªåŠ¨ç®¡ç†Cookie
                        self.is_authenticated = True
                        logger.info("è®¤è¯æˆåŠŸï¼Œsessionå·²è‡ªåŠ¨ç®¡ç†Cookie")
                        return True
                    elif response.status_code == 401:
                        logger.error("è®¤è¯å¤±è´¥: ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯")
                        return False
                    elif response.status_code == 429:
                        # é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…åé‡è¯•
                        retry_after = int(response.headers.get("Retry-After", auth_retry_delay))
                        logger.warning(f"è®¤è¯è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œç­‰å¾… {retry_after} ç§’åé‡è¯•...")
                        time.sleep(retry_after)
                        continue
                    else:
                        logger.error(f"è®¤è¯å¤±è´¥: çŠ¶æ€ç  {response.status_code}")
                        if auth_attempt < max_auth_retries - 1:
                            logger.info(f"{auth_retry_delay} ç§’åé‡è¯•...")
                            time.sleep(auth_retry_delay)
                            continue
                        else:
                            return False
                            
                except Exception as e:
                    logger.error(f"è®¤è¯è¯·æ±‚å¼‚å¸¸ (å°è¯• {auth_attempt + 1}/{max_auth_retries}): {e}")
                    if auth_attempt < max_auth_retries - 1:
                        logger.info(f"{auth_retry_delay} ç§’åé‡è¯•...")
                        time.sleep(auth_retry_delay)
                        continue
                    else:
                        return False
            
            return False
                
        except Exception as e:
            logger.error(f"è®¤è¯è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return False
    
    def setup_browser_headers(self):
        """è®¾ç½®æµè§ˆå™¨è¯·æ±‚å¤´ - åŸºäºå‚è€ƒæ–‡ä»¶çš„å®ç°"""
        # æ·»åŠ å¤šç§æµè§ˆå™¨æ ‡è¯†ï¼ŒåŒ…æ‹¬Chromeã€Firefoxã€Edgeç­‰
        browser_headers = [
            # Chromeæµè§ˆå™¨æ ‡è¯†
            {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',
                'Accept': 'application/json, text/plain, */*',
                'Accept-Encoding': 'gzip, deflate, br',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'Content-Type': 'application/json',
                'Origin': 'https://platform.worldquantbrain.com',
                'Pragma': 'no-cache',
                'Referer': 'https://platform.worldquantbrain.com/',
                'Sec-Ch-Ua': '\"Not?A_Brand\";v=\"99\", \"Chromium\";v=\"130\"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '\"Windows\"',
                'Sec-Fetch-Dest': 'empty',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Site': 'same-site'
            },
            # Firefoxæµè§ˆå™¨æ ‡è¯†
            {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:130.0) Gecko/20100101 Firefox/130.0',
                'Accept': 'application/json, text/plain, */*',
                'Accept-Encoding': 'gzip, deflate, br',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'Content-Type': 'application/json',
                'Origin': 'https://platform.worldquantbrain.com',
                'Pragma': 'no-cache',
                'Referer': 'https://platform.worldquantbrain.com/',
                'Sec-Fetch-Dest': 'empty',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Site': 'same-site'
            },
            # Edgeæµè§ˆå™¨æ ‡è¯†
            {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36 Edg/130.0.0.0',
                'Accept': 'application/json, text/plain, */*',
                'Accept-Encoding': 'gzip, deflate, br',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'Content-Type': 'application/json',
                'Origin': 'https://platform.worldquantbrain.com',
                'Pragma': 'no-cache',
                'Referer': 'https://platform.worldquantbrain.com/',
                'Sec-Ch-Ua': '\"Not?A_Brand\";v=\"99\", \"Microsoft Edge\";v=\"130\"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '\"Windows\"',
                'Sec-Fetch-Dest': 'empty',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Site': 'same-site'
            }
        ]
        
        # éšæœºé€‰æ‹©ä¸€ä¸ªæµè§ˆå™¨æ ‡è¯†
        selected_headers = random.choice(browser_headers)
        self.session.headers.update(selected_headers)
    
    def get_alphas(self, limit: int = 100, offset: int = 0, filters: Optional[Dict] = None) -> Optional[Dict]:
        """è·å–Alphaåˆ—è¡¨ - æ•´åˆAPIå®¢æˆ·ç«¯åŠŸèƒ½"""
        if not self.is_authenticated:
            logger.error("æœªè®¤è¯ï¼Œè¯·å…ˆè°ƒç”¨authenticateæ–¹æ³•")
            return None
        
        try:
            # æ„å»ºæŸ¥è¯¢å‚æ•°
            params = {
                'limit': limit,
                'offset': offset,
                'hidden': 'false',
                'order': '-dateCreated'
            }
            
            # æ·»åŠ è¿‡æ»¤æ¡ä»¶
            if filters:
                params.update(filters)
            
            # æ„å»ºURL
            query_string = urlencode(params, doseq=True)
            url = f"{self.base_url}/users/self/alphas?{query_string}"
            
            logger.info(f"è¯·æ±‚URL: {url}")
            
            response = self.session.get(url, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                logger.info(f"æˆåŠŸè·å–æ•°æ®ï¼Œcount: {data.get('count', 0)}, resultsæ•°é‡: {len(data.get('results', []))}")
                return data
            elif response.status_code == 429:
                logger.warning("APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾…åé‡è¯•")
                time.sleep(60)  # ç­‰å¾…1åˆ†é’Ÿåé‡è¯•
                return self.get_alphas(limit, offset, filters)
            else:
                logger.error(f"è·å–Alphaåˆ—è¡¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}")
                return None
                
        except requests.exceptions.RequestException as e:
            logger.error(f"è·å–Alphaåˆ—è¡¨è¯·æ±‚å¼‚å¸¸: {e}")
            return None
    
    def get_all_alphas(self, total_limit: Optional[int] = None, 
                      filters: Optional[Dict] = None) -> List[Dict]:
        """è·å–æ‰€æœ‰Alphaæ•°æ®ï¼ˆåˆ†é¡µè·å–ï¼‰ - æ•´åˆAPIå®¢æˆ·ç«¯åŠŸèƒ½"""
        if not self.is_authenticated:
            logger.error("æœªè®¤è¯ï¼Œè¯·å…ˆè°ƒç”¨authenticateæ–¹æ³•")
            return []
        
        all_alphas = []
        offset = 0
        limit = 100
        
        while True:
            logger.info(f"æ­£åœ¨è·å–ç¬¬ {offset//limit + 1} é¡µæ•°æ®...")
            
            data = self.get_alphas(limit, offset, filters)
            
            if not data:
                logger.error("è·å–æ•°æ®å¤±è´¥ï¼Œåœæ­¢è·å–")
                break
            
            results = data.get('results', [])
            if not results:
                logger.info("æ²¡æœ‰æ›´å¤šæ•°æ®")
                break
            
            all_alphas.extend(results)
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ€»æ•°é™åˆ¶
            if total_limit and len(all_alphas) >= total_limit:
                all_alphas = all_alphas[:total_limit]
                logger.info(f"è¾¾åˆ°æ€»æ•°é™åˆ¶ {total_limit}ï¼Œåœæ­¢è·å–")
                break
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
            next_url = data.get('next')
            if not next_url:
                logger.info("å·²è·å–æ‰€æœ‰æ•°æ®")
                break
            
            offset += limit
            
            # æ·»åŠ å»¶è¿Ÿé¿å…é¢‘ç¹è¯·æ±‚
            time.sleep(1)
        
        logger.info(f"æ€»å…±è·å– {len(all_alphas)} æ¡Alphaæ•°æ®")
        return all_alphas
    
    def test_connection(self) -> bool:
        """æµ‹è¯•è¿æ¥å’Œè®¤è¯ - æ•´åˆAPIå®¢æˆ·ç«¯åŠŸèƒ½"""
        logger.info("æµ‹è¯•APIè¿æ¥...")
        
        # å…ˆè®¤è¯
        if not self.authenticate():
            return False
        
        # æµ‹è¯•è·å–å°‘é‡æ•°æ®
        test_data = self.get_alphas(limit=1, offset=0)
        
        if test_data:
            logger.info("APIè¿æ¥æµ‹è¯•æˆåŠŸ")
            return True
        else:
            logger.error("APIè¿æ¥æµ‹è¯•å¤±è´¥")
            return False
    
    def connect_database(self) -> bool:
        """è¿æ¥æ•°æ®åº“"""
        try:
            config = self.load_config()
            db_config = config.get('database', {})
            
            self.db_connection = mysql.connector.connect(
                host=db_config.get('host', 'localhost'),
                port=db_config.get('port', 3306),
                user=db_config.get('username', 'quant_user'),
                password=db_config.get('password', 'quant_password'),
                database=db_config.get('database', 'consultant_analytics')
            )
            
            logger.info("æ•°æ®åº“è¿æ¥æˆåŠŸ")
            return True
            
        except Error as e:
            logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
    
    def create_tables(self) -> bool:
        """åˆ›å»ºæ•°æ®åº“è¡¨"""
        try:
            cursor = self.db_connection.cursor()
            
            # è¯»å–SQLæ–‡ä»¶
            sql_file = os.path.join(os.path.dirname(__file__), 'database_schema.sql')
            with open(sql_file, 'r', encoding='utf-8') as f:
                sql_script = f.read()
            
            # åˆ†å‰²SQLè¯­å¥å¹¶æ‰§è¡Œ
            statements = sql_script.split(';')
            for statement in statements:
                statement = statement.strip()
                if statement:
                    cursor.execute(statement)
            
            self.db_connection.commit()
            cursor.close()
            logger.info("æ•°æ®åº“è¡¨åˆ›å»ºæˆåŠŸ")
            return True
            
        except Error as e:
            logger.error(f"åˆ›å»ºæ•°æ®åº“è¡¨å¤±è´¥: {e}")
            return False
    
    def parse_datetime(self, datetime_str: Optional[str]) -> Optional[str]:
        """è§£ææ—¥æœŸæ—¶é—´ï¼Œè½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´å¹¶è¿”å›MySQLå…¼å®¹çš„å­—ç¬¦ä¸²æ ¼å¼"""
        if not datetime_str:
            return None
        
        try:
            # å¤„ç†ISO 8601æ ¼å¼ï¼ŒåŒ…å«æ—¶åŒºä¿¡æ¯
            if 'T' in datetime_str:
                # ä½¿ç”¨dateutil.parserå¤„ç†å„ç§ISOæ ¼å¼
                from dateutil import parser
                from dateutil import tz
                dt = parser.isoparse(datetime_str)
                # å¦‚æœæ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼Œå‡è®¾ä¸ºUTC
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=tz.tzutc())
                # è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´ (UTC+8)
                beijing_tz = tz.gettz('Asia/Shanghai')
                dt_beijing = dt.astimezone(beijing_tz)
                # è½¬æ¢ä¸ºMySQLå…¼å®¹çš„datetimeæ ¼å¼
                return dt_beijing.strftime('%Y-%m-%d %H:%M:%S')
            else:
                # çº¯æ—¥æœŸæ ¼å¼ï¼Œè½¬æ¢ä¸ºdatetimeæ ¼å¼
                dt = datetime.strptime(datetime_str, '%Y-%m-%d')
                return dt.strftime('%Y-%m-%d %H:%M:%S')
        except Exception as e:
            logger.warning(f"æ—¥æœŸæ—¶é—´è§£æå¤±è´¥: {datetime_str}, é”™è¯¯: {e}")
            return None
    
    def parse_date(self, date_str: Optional[str]) -> Optional[str]:
        """è§£ææ—¥æœŸï¼Œè¿”å›MySQLå…¼å®¹çš„æ—¥æœŸå­—ç¬¦ä¸²æ ¼å¼"""
        if not date_str:
            return None
        
        try:
            # è§£ææ—¥æœŸå¹¶è¿”å›æ ‡å‡†æ ¼å¼
            dt = datetime.strptime(date_str, '%Y-%m-%d')
            return dt.strftime('%Y-%m-%d')
        except Exception as e:
            logger.warning(f"æ—¥æœŸè§£æå¤±è´¥: {date_str}, é”™è¯¯: {e}")
            return None
    
    def parse_alpha_data(self, alpha_data: Dict) -> Dict:
        """è§£æAlphaæ•°æ®"""
        try:
            # å¤„ç†nameå­—æ®µä¸ºç©ºçš„æƒ…å†µï¼Œç›´æ¥å¡«å…¥"anonymous"
            alpha_name = alpha_data.get('name')
            if not alpha_name:
                alpha_name = "anonymous"
            
            # åŸºæœ¬ä¿¡æ¯
            parsed = {
                'id': alpha_data.get('id'),
                'type': alpha_data.get('type'),
                'author': alpha_data.get('author'),
                'date_created': self.parse_datetime(alpha_data.get('dateCreated')),
                'date_submitted': self.parse_datetime(alpha_data.get('dateSubmitted')),
                'date_modified': self.parse_datetime(alpha_data.get('dateModified')),
                'name': alpha_name,
                'favorite': alpha_data.get('favorite', False),
                'hidden': alpha_data.get('hidden', False),
                'color': alpha_data.get('color'),
                'category': alpha_data.get('category'),
                'stage': alpha_data.get('stage'),
                'status': alpha_data.get('status'),
                'grade': alpha_data.get('grade'),
            }
            
            # è®¾ç½®ä¿¡æ¯
            settings = alpha_data.get('settings', {})
            parsed.update({
                'instrument_type': settings.get('instrumentType'),
                'region': settings.get('region'),
                'universe': settings.get('universe'),
                'delay': settings.get('delay'),
                'decay': settings.get('decay'),
                'neutralization': settings.get('neutralization'),
                'truncation': settings.get('truncation'),
                'pasteurization': settings.get('pasteurization'),
                'unit_handling': settings.get('unitHandling'),
                'nan_handling': settings.get('nanHandling'),
                'selection_handling': settings.get('selectionHandling'),  # SUPERç±»å‹ç‰¹æœ‰å­—æ®µ
                'selection_limit': settings.get('selectionLimit'),       # SUPERç±»å‹ç‰¹æœ‰å­—æ®µ
                'max_trade': settings.get('maxTrade'),
                'language': settings.get('language'),
                'visualization': settings.get('visualization'),
                'start_date': self.parse_date(settings.get('startDate')),
                'end_date': self.parse_date(settings.get('endDate')),
                'component_activation': settings.get('componentActivation'),  # SUPERç±»å‹ç‰¹æœ‰å­—æ®µ
                'test_period': settings.get('testPeriod'),                    # SUPERç±»å‹ç‰¹æœ‰å­—æ®µ
            })
            
            # å¸¸è§„ä¿¡æ¯ - æ”¯æŒREGULARå’ŒSUPERç±»å‹
            # å¯¹äºSUPERç±»å‹ï¼Œæˆ‘ä»¬éœ€è¦ä»comboå’Œselectionä¸­æå–ä¿¡æ¯
            # ä¼˜å…ˆçº§ï¼šregular.code > combo.code > selection.code
            alpha_type = alpha_data.get('type', 'REGULAR')
            
            if alpha_type == 'SUPER':
                # å¤„ç†SUPERç±»å‹
                combo = alpha_data.get('combo', {})
                selection = alpha_data.get('selection', {})
                
                # åˆå¹¶ç­–ç•¥ï¼šå°†comboå’Œselectionçš„ä¿¡æ¯éƒ½ä¿å­˜ä¸‹æ¥
                # ä»£ç å­—æ®µåˆå¹¶ï¼š[combo_code: XXXX, selection_code: xxxxx]
                combo_code = combo.get('code')
                selection_code = selection.get('code')
                if combo_code and selection_code:
                    code = f"[combo_code: {combo_code}, selection_code: {selection_code}]"
                elif combo_code:
                    code = f"[combo_code: {combo_code}]"
                elif selection_code:
                    code = f"[selection_code: {selection_code}]"
                else:
                    code = None
                
                # æè¿°å­—æ®µåˆå¹¶ï¼š[combo_description: XXXX, selection_description: xxxxx]
                combo_description = combo.get('description')
                selection_description = selection.get('description')
                if combo_description and selection_description:
                    description = f"[combo_description: {combo_description}, selection_description: {selection_description}]"
                elif combo_description:
                    description = f"[combo_description: {combo_description}]"
                elif selection_description:
                    description = f"[selection_description: {selection_description}]"
                else:
                    description = None
                
                # æ“ä½œç¬¦è®¡æ•°åˆå¹¶ï¼š[combo_operator_count: X, selection_operator_count: Y]
                combo_operator_count = combo.get('operatorCount')
                selection_operator_count = selection.get('operatorCount')
                if combo_operator_count is not None and selection_operator_count is not None:
                    operator_count = f"[combo_operator_count: {combo_operator_count}, selection_operator_count: {selection_operator_count}]"
                elif combo_operator_count is not None:
                    operator_count = f"[combo_operator_count: {combo_operator_count}]"
                elif selection_operator_count is not None:
                    operator_count = f"[selection_operator_count: {selection_operator_count}]"
                else:
                    operator_count = None
                
                # æå–comboä¿¡æ¯
                combo_code = combo.get('code')
                combo_description = combo.get('description')
                combo_operator_count = combo.get('operatorCount')
                
                # æå–selectionä¿¡æ¯
                selection_code = selection.get('code')
                selection_description = selection.get('description')
                selection_operator_count = selection.get('operatorCount')
                
                parsed.update({
                    'code': code,
                    'description': description,
                    'operator_count': operator_count,
                    # æ–°å¢comboå’Œselectionçš„å®Œæ•´ä¿¡æ¯
                    'combo_code': combo_code,
                    'combo_description': combo_description,
                    'combo_operator_count': combo_operator_count,
                    'selection_code': selection_code,
                    'selection_description': selection_description,
                    'selection_operator_count': selection_operator_count,
                })
            else:
                # å¤„ç†REGULARç±»å‹
                regular = alpha_data.get('regular', {})
                parsed.update({
                    'code': regular.get('code'),
                    'description': regular.get('description'),
                    'operator_count': regular.get('operatorCount'),
                })
            
            # æ ‡ç­¾å’Œåˆ†ç±»ä¿¡æ¯
            parsed.update({
                'tags': json.dumps(alpha_data.get('tags', [])),
                'classifications': json.dumps(alpha_data.get('classifications', [])),
            })
            
            # ISé˜¶æ®µæ€§èƒ½æŒ‡æ ‡
            is_data = alpha_data.get('is', {})
            parsed.update({
                'pnl': is_data.get('pnl'),
                'book_size': is_data.get('bookSize'),
                'long_count': is_data.get('longCount'),
                'short_count': is_data.get('shortCount'),
                'turnover': is_data.get('turnover'),
                'returns': is_data.get('returns'),
                'drawdown': is_data.get('drawdown'),
                'margin': is_data.get('margin'),
                'sharpe': is_data.get('sharpe'),
                'fitness': is_data.get('fitness'),
                'is_start_date': self.parse_date(is_data.get('startDate')),
            })
            
            # æŠ•èµ„çº¦æŸæ€§èƒ½æŒ‡æ ‡
            invest_constrained = is_data.get('investabilityConstrained', {})
            parsed.update({
                'investability_constrained_pnl': invest_constrained.get('pnl'),
                'investability_constrained_book_size': invest_constrained.get('bookSize'),
                'investability_constrained_long_count': invest_constrained.get('longCount'),
                'investability_constrained_short_count': invest_constrained.get('shortCount'),
                'investability_constrained_turnover': invest_constrained.get('turnover'),
                'investability_constrained_returns': invest_constrained.get('returns'),
                'investability_constrained_drawdown': invest_constrained.get('drawdown'),
                'investability_constrained_margin': invest_constrained.get('margin'),
                'investability_constrained_fitness': invest_constrained.get('fitness'),
                'investability_constrained_sharpe': invest_constrained.get('sharpe'),
            })
            
            # é£é™©ä¸­æ€§åŒ–æ€§èƒ½æŒ‡æ ‡
            risk_neutralized = is_data.get('riskNeutralized', {})
            parsed.update({
                'risk_neutralized_pnl': risk_neutralized.get('pnl'),
                'risk_neutralized_book_size': risk_neutralized.get('bookSize'),
                'risk_neutralized_long_count': risk_neutralized.get('longCount'),
                'risk_neutralized_short_count': risk_neutralized.get('shortCount'),
                'risk_neutralized_turnover': risk_neutralized.get('turnover'),
                'risk_neutralized_returns': risk_neutralized.get('returns'),
                'risk_neutralized_drawdown': risk_neutralized.get('drawdown'),
                'risk_neutralized_margin': risk_neutralized.get('margin'),
                'risk_neutralized_fitness': risk_neutralized.get('fitness'),
                'risk_neutralized_sharpe': risk_neutralized.get('sharpe'),
            })
            
            # å…¶ä»–ä¿¡æ¯
            parsed.update({
                'checks': json.dumps(is_data.get('checks', [])),
                'competitions': json.dumps(alpha_data.get('competitions', [])),
                'pyramids': json.dumps(alpha_data.get('pyramids', [])),
                'themes': json.dumps(alpha_data.get('themes', [])),
            })
            
            return parsed
            
        except Exception as e:
            logger.error(f"è§£æAlphaæ•°æ®å¤±è´¥: {e}")
            return {}
    
    def save_alpha_to_database(self, alpha_data: Dict) -> bool:
        """ä¿å­˜Alphaæ•°æ®åˆ°æ•°æ®åº“"""
        try:
            cursor = self.db_connection.cursor()
            
            # å‡†å¤‡æ•°æ®ï¼Œå¤„ç†Noneå€¼
            data = {
                'id': alpha_data.get('id') or 'NULL',
                'type': alpha_data.get('type') or 'NULL',
                'author': alpha_data.get('author') or 'NULL',
                'date_created': alpha_data.get('date_created') or 'NULL',
                'date_submitted': alpha_data.get('date_submitted') or 'NULL',
                'date_modified': alpha_data.get('date_modified') or 'NULL',
                'name': alpha_data.get('name') or 'NULL',
                'favorite': alpha_data.get('favorite') or 'NULL',
                'hidden': alpha_data.get('hidden') or 'NULL',
                'color': alpha_data.get('color') or 'NULL',
                'category': alpha_data.get('category') or 'NULL',
                'stage': alpha_data.get('stage') or 'NULL',
                'status': alpha_data.get('status') or 'NULL',
                'grade': alpha_data.get('grade') or 'NULL',
                'instrument_type': alpha_data.get('instrument_type') or 'NULL',
                'region': alpha_data.get('region') or 'NULL',
                'universe': alpha_data.get('universe') or 'NULL',
                'delay': alpha_data.get('delay') if alpha_data.get('delay') is not None else 'NULL',
                'decay': alpha_data.get('decay') if alpha_data.get('decay') is not None else 'NULL',
                'neutralization': alpha_data.get('neutralization') or 'NULL',
                'truncation': alpha_data.get('truncation') if alpha_data.get('truncation') is not None else 'NULL',
                'pasteurization': alpha_data.get('pasteurization') or 'NULL',
                'unit_handling': alpha_data.get('unit_handling') or 'NULL',
                'nan_handling': alpha_data.get('nan_handling') or 'NULL',
                'selection_handling': alpha_data.get('selection_handling') if alpha_data.get('selection_handling') is not None else 'NULL',  # SUPERç±»å‹ç‰¹æœ‰å­—æ®µ
                'selection_limit': alpha_data.get('selection_limit') if alpha_data.get('selection_limit') is not None else 'NULL',       # SUPERç±»å‹ç‰¹æœ‰å­—æ®µ
                'max_trade': alpha_data.get('max_trade') or 'NULL',
                'language': alpha_data.get('language') or 'NULL',
                'visualization': alpha_data.get('visualization') or 'NULL',
                'start_date': alpha_data.get('start_date') or 'NULL',
                'end_date': alpha_data.get('end_date') or 'NULL',
                'component_activation': alpha_data.get('component_activation') if alpha_data.get('component_activation') is not None else 'NULL',  # SUPERç±»å‹ç‰¹æœ‰å­—æ®µ
                'test_period': alpha_data.get('test_period') if alpha_data.get('test_period') is not None else 'NULL',                    # SUPERç±»å‹ç‰¹æœ‰å­—æ®µ
                'code': alpha_data.get('code') or 'NULL',
                'description': alpha_data.get('description') or 'NULL',
                'operator_count': alpha_data.get('operator_count') or 'NULL',
                # æ–°å¢çš„comboå’Œselectionå­—æ®µ
                'combo_code': alpha_data.get('combo_code') or 'NULL',
                'combo_description': alpha_data.get('combo_description') or 'NULL',
                'combo_operator_count': alpha_data.get('combo_operator_count') if alpha_data.get('combo_operator_count') is not None else 'NULL',
                'selection_code': alpha_data.get('selection_code') or 'NULL',
                'selection_description': alpha_data.get('selection_description') or 'NULL',
                'selection_operator_count': alpha_data.get('selection_operator_count') if alpha_data.get('selection_operator_count') is not None else 'NULL',
                'tags': alpha_data.get('tags') or 'NULL',
                'classifications': alpha_data.get('classifications') or 'NULL',
                'pnl': alpha_data.get('pnl') if alpha_data.get('pnl') is not None else 'NULL',
                'book_size': alpha_data.get('book_size') if alpha_data.get('book_size') is not None else 'NULL',
                'long_count': alpha_data.get('long_count') if alpha_data.get('long_count') is not None else 'NULL',
                'short_count': alpha_data.get('short_count') if alpha_data.get('short_count') is not None else 'NULL',
                'turnover': alpha_data.get('turnover') if alpha_data.get('turnover') is not None else 'NULL',
                'returns': alpha_data.get('returns') if alpha_data.get('returns') is not None else 'NULL',
                'drawdown': alpha_data.get('drawdown') if alpha_data.get('drawdown') is not None else 'NULL',
                'margin': alpha_data.get('margin') if alpha_data.get('margin') is not None else 'NULL',
                'sharpe': alpha_data.get('sharpe') if alpha_data.get('sharpe') is not None else 'NULL',
                'fitness': alpha_data.get('fitness') if alpha_data.get('fitness') is not None else 'NULL',
                'is_start_date': alpha_data.get('is_start_date') or 'NULL',
                'investability_constrained_pnl': alpha_data.get('investability_constrained_pnl') if alpha_data.get('investability_constrained_pnl') is not None else 'NULL',
                'investability_constrained_book_size': alpha_data.get('investability_constrained_book_size') if alpha_data.get('investability_constrained_book_size') is not None else 'NULL',
                'investability_constrained_long_count': alpha_data.get('investability_constrained_long_count') if alpha_data.get('investability_constrained_long_count') is not None else 'NULL',
                'investability_constrained_short_count': alpha_data.get('investability_constrained_short_count') if alpha_data.get('investability_constrained_short_count') is not None else 'NULL',
                'investability_constrained_turnover': alpha_data.get('investability_constrained_turnover') if alpha_data.get('investability_constrained_turnover') is not None else 'NULL',
                'investability_constrained_returns': alpha_data.get('investability_constrained_returns') if alpha_data.get('investability_constrained_returns') is not None else 'NULL',
                'investability_constrained_drawdown': alpha_data.get('investability_constrained_drawdown') if alpha_data.get('investability_constrained_drawdown') is not None else 'NULL',
                'investability_constrained_margin': alpha_data.get('investability_constrained_margin') if alpha_data.get('investability_constrained_margin') is not None else 'NULL',
                'investability_constrained_fitness': alpha_data.get('investability_constrained_fitness') if alpha_data.get('investability_constrained_fitness') is not None else 'NULL',
                'investability_constrained_sharpe': alpha_data.get('investability_constrained_sharpe') if alpha_data.get('investability_constrained_sharpe') is not None else 'NULL',
                'risk_neutralized_pnl': alpha_data.get('risk_neutralized_pnl') if alpha_data.get('risk_neutralized_pnl') is not None else 'NULL',
                'risk_neutralized_book_size': alpha_data.get('risk_neutralized_book_size') if alpha_data.get('risk_neutralized_book_size') is not None else 'NULL',
                'risk_neutralized_long_count': alpha_data.get('risk_neutralized_long_count') if alpha_data.get('risk_neutralized_long_count') is not None else 'NULL',
                'risk_neutralized_short_count': alpha_data.get('risk_neutralized_short_count') if alpha_data.get('risk_neutralized_short_count') is not None else 'NULL',
                'risk_neutralized_turnover': alpha_data.get('risk_neutralized_turnover') if alpha_data.get('risk_neutralized_turnover') is not None else 'NULL',
                'risk_neutralized_returns': alpha_data.get('risk_neutralized_returns') if alpha_data.get('risk_neutralized_returns') is not None else 'NULL',
                'risk_neutralized_drawdown': alpha_data.get('risk_neutralized_drawdown') if alpha_data.get('risk_neutralized_drawdown') is not None else 'NULL',
                'risk_neutralized_margin': alpha_data.get('risk_neutralized_margin') if alpha_data.get('risk_neutralized_margin') is not None else 'NULL',
                'risk_neutralized_fitness': alpha_data.get('risk_neutralized_fitness') if alpha_data.get('risk_neutralized_fitness') is not None else 'NULL',
                'risk_neutralized_sharpe': alpha_data.get('risk_neutralized_sharpe') if alpha_data.get('risk_neutralized_sharpe') is not None else 'NULL',
                'checks': alpha_data.get('checks') or 'NULL',
                'competitions': alpha_data.get('competitions') or 'NULL',
                'pyramids': alpha_data.get('pyramids') or 'NULL',
                'themes': alpha_data.get('themes') or 'NULL'
            }
            
            # è½¬ä¹‰å•å¼•å·å¹¶åŒ…è£…å­—ç¬¦ä¸²å€¼
            # JSONå­—æ®µä¸éœ€è¦é¢å¤–çš„è½¬ä¹‰å¤„ç†
            json_fields = {'checks', 'competitions', 'pyramids', 'themes', 'tags', 'classifications'}
            
            for key, value in data.items():
                if value == 'NULL':
                    data[key] = None  # å°†'NULL'å­—ç¬¦ä¸²æ”¹ä¸ºNoneï¼Œè®©å‚æ•°åŒ–æŸ¥è¯¢æ­£ç¡®å¤„ç†NULLå€¼
                elif key in ['date_created', 'date_submitted', 'date_modified']:
                    # æ—¥æœŸæ—¶é—´å­—æ®µå·²ç»å¤„ç†è¿‡ï¼Œä¸éœ€è¦å†åŒ…è£…ï¼ˆä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢ï¼‰
                    if value is not None:
                        data[key] = str(value)
                    else:
                        data[key] = None  # å°†'NULL'å­—ç¬¦ä¸²æ”¹ä¸ºNoneï¼Œè®©å‚æ•°åŒ–æŸ¥è¯¢æ­£ç¡®å¤„ç†NULLå€¼
                elif key in ['start_date', 'end_date', 'is_start_date']:
                    # æ—¥æœŸå­—æ®µå·²ç»å¤„ç†è¿‡ï¼Œä¸éœ€è¦å†åŒ…è£…ï¼ˆä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢ï¼‰
                    if value is not None:
                        data[key] = str(value)
                    else:
                        data[key] = None  # å°†'NULL'å­—ç¬¦ä¸²æ”¹ä¸ºNoneï¼Œè®©å‚æ•°åŒ–æŸ¥è¯¢æ­£ç¡®å¤„ç†NULLå€¼
                elif key in json_fields:
                    # JSONå­—æ®µç‰¹æ®Šå¤„ç†ï¼Œåªéœ€è¦ç¡®ä¿æ˜¯æœ‰æ•ˆçš„JSONå­—ç¬¦ä¸²
                    if isinstance(value, str) and value != 'NULL':
                        # JSONå­—æ®µéœ€è¦è½¬ä¹‰å•å¼•å·ï¼Œä½†ä¸èƒ½è½¬ä¹‰åŒå¼•å·ï¼ˆJSONä¸­çš„åŒå¼•å·æ˜¯æœ‰æ•ˆçš„ï¼‰
                        # ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢æ¥æ­£ç¡®å¤„ç†JSONå­—æ®µï¼Œé¿å…æ‰‹åŠ¨è½¬ä¹‰
                        data[key] = value
                    elif value == 'NULL':
                        data[key] = None  # å°†'NULL'å­—ç¬¦ä¸²æ”¹ä¸ºNoneï¼Œè®©å‚æ•°åŒ–æŸ¥è¯¢æ­£ç¡®å¤„ç†NULLå€¼
                    else:
                        # å…¶ä»–æƒ…å†µè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        data[key] = str(value)
                elif isinstance(value, str):
                    # æ™®é€šå­—ç¬¦ä¸²å­—æ®µï¼Œä¸è¿›è¡Œå•å¼•å·åŒ…è£…ï¼ˆç”±å‚æ•°åŒ–æŸ¥è¯¢å¤„ç†ï¼‰
                    # åªéœ€è¦ç¡®ä¿å­—ç¬¦ä¸²æœ¬èº«æ˜¯æœ‰æ•ˆçš„ï¼Œä¸éœ€è¦æ‰‹åŠ¨è½¬ä¹‰å•å¼•å·
                    pass  # ä¾èµ–å‚æ•°åŒ–æŸ¥è¯¢è‡ªåŠ¨å¤„ç†å­—ç¬¦ä¸²è½¬ä¹‰å’ŒåŒ…è£…
                elif isinstance(value, (int, float)):
                    # æ•°å€¼ç±»å‹ä¿æŒåŸæ ·ï¼Œè®©å‚æ•°åŒ–æŸ¥è¯¢å¤„ç†
                    pass  # ä¸éœ€è¦è½¬æ¢ï¼Œä¿æŒåŸå§‹æ•°å€¼ç±»å‹
                elif isinstance(value, bool):
                    # å¸ƒå°”å€¼ä¿æŒåŸæ ·ï¼Œè®©å‚æ•°åŒ–æŸ¥è¯¢å¤„ç†
                    pass  # ä¸éœ€è¦è½¬æ¢ï¼Œä¿æŒåŸå§‹å¸ƒå°”ç±»å‹
                elif value is None:
                    data[key] = None  # å°†'NULL'å­—ç¬¦ä¸²æ”¹ä¸ºNoneï¼Œè®©å‚æ•°åŒ–æŸ¥è¯¢æ­£ç¡®å¤„ç†NULLå€¼
                else:
                    # å…¶ä»–ç±»å‹ä¿æŒåŸæ ·æˆ–è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    pass  # å¯¹äºå…¶ä»–ç±»å‹ï¼Œä¾èµ–å‚æ•°åŒ–æŸ¥è¯¢çš„è‡ªåŠ¨å¤„ç†
            
            sql_template = """
            INSERT INTO alphas (
                id, type, author, date_created, date_submitted, date_modified, name,
                favorite, hidden, color, category, stage, status, grade,
                instrument_type, region, universe, delay, decay, neutralization,
                truncation, pasteurization, unit_handling, nan_handling, selection_handling, selection_limit,
                max_trade, language, visualization, start_date, end_date, component_activation, test_period,
                code, description, operator_count,
                combo_code, combo_description, combo_operator_count,
                selection_code, selection_description, selection_operator_count,
                tags, classifications,
                pnl, book_size, long_count, short_count, turnover, returns, drawdown,
                margin, sharpe, fitness, is_start_date,
                investability_constrained_pnl, investability_constrained_book_size,
                investability_constrained_long_count, investability_constrained_short_count,
                investability_constrained_turnover, investability_constrained_returns,
                investability_constrained_drawdown, investability_constrained_margin,
                investability_constrained_fitness, investability_constrained_sharpe,
                risk_neutralized_pnl, risk_neutralized_book_size,
                risk_neutralized_long_count, risk_neutralized_short_count,
                risk_neutralized_turnover, risk_neutralized_returns,
                risk_neutralized_drawdown, risk_neutralized_margin,
                risk_neutralized_fitness, risk_neutralized_sharpe,
                checks, competitions, pyramids, themes
            ) VALUES (
                {id}, {type}, {author}, {date_created}, {date_submitted}, {date_modified}, {name},
                {favorite}, {hidden}, {color}, {category}, {stage}, {status}, {grade},
                {instrument_type}, {region}, {universe}, {delay}, {decay}, {neutralization},
                {truncation}, {pasteurization}, {unit_handling}, {nan_handling}, {selection_handling}, {selection_limit},
                {max_trade}, {language}, {visualization}, {start_date}, {end_date}, {component_activation}, {test_period},
                {code}, {description}, {operator_count},
                {combo_code}, {combo_description}, {combo_operator_count},
                {selection_code}, {selection_description}, {selection_operator_count},
                {tags}, {classifications},
                {pnl}, {book_size}, {long_count}, {short_count}, {turnover}, {returns}, {drawdown},
                {margin}, {sharpe}, {fitness}, {is_start_date},
                {investability_constrained_pnl}, {investability_constrained_book_size},
                {investability_constrained_long_count}, {investability_constrained_short_count},
                {investability_constrained_turnover}, {investability_constrained_returns},
                {investability_constrained_drawdown}, {investability_constrained_margin},
                {investability_constrained_fitness}, {investability_constrained_sharpe},
                {risk_neutralized_pnl}, {risk_neutralized_book_size},
                {risk_neutralized_long_count}, {risk_neutralized_short_count},
                {risk_neutralized_turnover}, {risk_neutralized_returns},
                {risk_neutralized_drawdown}, {risk_neutralized_margin},
                {risk_neutralized_fitness}, {risk_neutralized_sharpe},
                {checks}, {competitions}, {pyramids}, {themes}
            ) ON DUPLICATE KEY UPDATE
                type = VALUES(type), author = VALUES(author), date_created = VALUES(date_created),
                date_submitted = VALUES(date_submitted), date_modified = VALUES(date_modified),
                name = VALUES(name), favorite = VALUES(favorite), hidden = VALUES(hidden),
                color = VALUES(color), category = VALUES(category), stage = VALUES(stage),
                status = VALUES(status), grade = VALUES(grade), instrument_type = VALUES(instrument_type),
                region = VALUES(region), universe = VALUES(universe), delay = VALUES(delay),
                decay = VALUES(decay), neutralization = VALUES(neutralization), truncation = VALUES(truncation),
                pasteurization = VALUES(pasteurization), unit_handling = VALUES(unit_handling),
                nan_handling = VALUES(nan_handling), max_trade = VALUES(max_trade), language = VALUES(language),
                visualization = VALUES(visualization), start_date = VALUES(start_date), end_date = VALUES(end_date),
                code = VALUES(code), description = VALUES(description), operator_count = VALUES(operator_count),
                combo_code = VALUES(combo_code), combo_description = VALUES(combo_description), combo_operator_count = VALUES(combo_operator_count),
                selection_code = VALUES(selection_code), selection_description = VALUES(selection_description), selection_operator_count = VALUES(selection_operator_count),
                tags = VALUES(tags), classifications = VALUES(classifications), pnl = VALUES(pnl),
                book_size = VALUES(book_size), long_count = VALUES(long_count), short_count = VALUES(short_count),
                turnover = VALUES(turnover), returns = VALUES(returns), drawdown = VALUES(drawdown),
                margin = VALUES(margin), sharpe = VALUES(sharpe), fitness = VALUES(fitness),
                is_start_date = VALUES(is_start_date), investability_constrained_pnl = VALUES(investability_constrained_pnl),
                investability_constrained_book_size = VALUES(investability_constrained_book_size),
                investability_constrained_long_count = VALUES(investability_constrained_long_count),
                investability_constrained_short_count = VALUES(investability_constrained_short_count),
                investability_constrained_turnover = VALUES(investability_constrained_turnover),
                investability_constrained_returns = VALUES(investability_constrained_returns),
                investability_constrained_drawdown = VALUES(investability_constrained_drawdown),
                investability_constrained_margin = VALUES(investability_constrained_margin),
                investability_constrained_fitness = VALUES(investability_constrained_fitness),
                investability_constrained_sharpe = VALUES(investability_constrained_sharpe),
                risk_neutralized_pnl = VALUES(risk_neutralized_pnl),
                risk_neutralized_book_size = VALUES(risk_neutralized_book_size),
                risk_neutralized_long_count = VALUES(risk_neutralized_long_count),
                risk_neutralized_short_count = VALUES(risk_neutralized_short_count),
                risk_neutralized_turnover = VALUES(risk_neutralized_turnover),
                risk_neutralized_returns = VALUES(risk_neutralized_returns),
                risk_neutralized_drawdown = VALUES(risk_neutralized_drawdown),
                risk_neutralized_margin = VALUES(risk_neutralized_margin),
                risk_neutralized_fitness = VALUES(risk_neutralized_fitness),
                risk_neutralized_sharpe = VALUES(risk_neutralized_sharpe),
                checks = VALUES(checks), competitions = VALUES(competitions), pyramids = VALUES(pyramids),
                themes = VALUES(themes), updated_at = CURRENT_TIMESTAMP
            """
            
            # æå–æ‰€æœ‰å€¼ä½œä¸ºå‚æ•°å…ƒç»„ï¼Œä¿æŒä¸SQLæ¨¡æ¿ä¸­å ä½ç¬¦çš„é¡ºåºä¸€è‡´
            values = (
                data['id'], data['type'], data['author'], data['date_created'], data['date_submitted'], data['date_modified'], data['name'],
                data['favorite'], data['hidden'], data['color'], data['category'], data['stage'], data['status'], data['grade'],
                data['instrument_type'], data['region'], data['universe'], data['delay'], data['decay'], data['neutralization'],
                data['truncation'], data['pasteurization'], data['unit_handling'], data['nan_handling'], data['selection_handling'], data['selection_limit'],
                data['max_trade'], data['language'], data['visualization'], data['start_date'], data['end_date'], data['component_activation'], data['test_period'],
                data['code'], data['description'], data['operator_count'],
                data['combo_code'], data['combo_description'], data['combo_operator_count'],
                data['selection_code'], data['selection_description'], data['selection_operator_count'],
                data['tags'], data['classifications'],
                data['pnl'], data['book_size'], data['long_count'], data['short_count'], data['turnover'], data['returns'], data['drawdown'],
                data['margin'], data['sharpe'], data['fitness'], data['is_start_date'],
                data['investability_constrained_pnl'], data['investability_constrained_book_size'],
                data['investability_constrained_long_count'], data['investability_constrained_short_count'],
                data['investability_constrained_turnover'], data['investability_constrained_returns'],
                data['investability_constrained_drawdown'], data['investability_constrained_margin'],
                data['investability_constrained_fitness'], data['investability_constrained_sharpe'],
                data['risk_neutralized_pnl'], data['risk_neutralized_book_size'],
                data['risk_neutralized_long_count'], data['risk_neutralized_short_count'],
                data['risk_neutralized_turnover'], data['risk_neutralized_returns'],
                data['risk_neutralized_drawdown'], data['risk_neutralized_margin'],
                data['risk_neutralized_fitness'], data['risk_neutralized_sharpe'],
                data['checks'], data['competitions'], data['pyramids'], data['themes']
            )
            
            # ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢æ‰§è¡ŒSQLè¯­å¥
            sql = sql_template.format(
                id='%s', type='%s', author='%s', date_created='%s', date_submitted='%s', date_modified='%s', name='%s',
                favorite='%s', hidden='%s', color='%s', category='%s', stage='%s', status='%s', grade='%s',
                instrument_type='%s', region='%s', universe='%s', delay='%s', decay='%s', neutralization='%s',
                truncation='%s', pasteurization='%s', unit_handling='%s', nan_handling='%s', selection_handling='%s', selection_limit='%s',
                max_trade='%s', language='%s', visualization='%s', start_date='%s', end_date='%s', component_activation='%s', test_period='%s',
                code='%s', description='%s', operator_count='%s',
                combo_code='%s', combo_description='%s', combo_operator_count='%s',
                selection_code='%s', selection_description='%s', selection_operator_count='%s',
                tags='%s', classifications='%s',
                pnl='%s', book_size='%s', long_count='%s', short_count='%s', turnover='%s', returns='%s', drawdown='%s',
                margin='%s', sharpe='%s', fitness='%s', is_start_date='%s',
                investability_constrained_pnl='%s', investability_constrained_book_size='%s',
                investability_constrained_long_count='%s', investability_constrained_short_count='%s',
                investability_constrained_turnover='%s', investability_constrained_returns='%s',
                investability_constrained_drawdown='%s', investability_constrained_margin='%s',
                investability_constrained_fitness='%s', investability_constrained_sharpe='%s',
                risk_neutralized_pnl='%s', risk_neutralized_book_size='%s',
                risk_neutralized_long_count='%s', risk_neutralized_short_count='%s',
                risk_neutralized_turnover='%s', risk_neutralized_returns='%s',
                risk_neutralized_drawdown='%s', risk_neutralized_margin='%s',
                risk_neutralized_fitness='%s', risk_neutralized_sharpe='%s',
                checks='%s', competitions='%s', pyramids='%s', themes='%s'
            )
            
            # è®°å½•å®Œæ•´çš„SQLè¯­å¥ä»¥ä¾¿è°ƒè¯•
            logger.debug(f"å‡†å¤‡æ‰§è¡Œçš„å®Œæ•´SQLè¯­å¥: {sql}")
            
            cursor.execute(sql, values)
            self.db_connection.commit()
            cursor.close()
            
            # logger.info(f"ä¿å­˜Alphaæ•°æ®æˆåŠŸ: {alpha_data.get('id')}")
            return True
            
        except Error as e:
            alpha_id = alpha_data.get('id', 'æœªçŸ¥ID')
            # æ‰“å°å®Œæ•´SQLè¯­å¥ä»¥ä¾¿è°ƒè¯•
            logger.error(f"ä¿å­˜Alphaæ•°æ®å¤±è´¥ (ID: {alpha_id}): {e}")
            # åœ¨é”™è¯¯æ—¥å¿—ä¸­æ˜¾ç¤ºå®é™…æ‰§è¡Œçš„å®Œæ•´SQLè¯­å¥
            logger.error(f"å®Œæ•´SQLè¯­å¥: {sql}")
            return False
    
    def get_alphas_page(self, limit: int = 100, offset: int = 0, filters: Optional[Dict] = None) -> Optional[Dict]:
        """è·å–ä¸€é¡µAlphaæ•°æ®"""
        if not self.is_authenticated:
            logger.error("æœªè®¤è¯ï¼Œè¯·å…ˆè°ƒç”¨authenticateæ–¹æ³•")
            return None
        
        try:
            # æ„å»ºåŸºç¡€URL
            api_url = f"{self.base_url}/users/self/alphas?limit={limit}&offset={offset}&hidden=false&order=-dateCreated"
            
            # æ·»åŠ è¿‡æ»¤æ¡ä»¶
            if filters:
                for key, value in filters.items():
                    # æ­£ç¡®å¤„ç†å‚æ•°é”®ç¼–ç ï¼šåªå¯¹>å’Œ<è¿›è¡Œç¼–ç 
                    # éœ€è¦ç‰¹åˆ«å¤„ç†>=å’Œ<=çš„æƒ…å†µï¼Œåªç¼–ç >å’Œ<å­—ç¬¦ï¼Œä¿ç•™=ä½œä¸ºé”®å€¼åˆ†éš”ç¬¦
                    if '>=' in key:
                        encoded_key = key.replace('>=', '%3E')
                    elif '<=' in key:
                        encoded_key = key.replace('<=', '%3C')
                    else:
                        encoded_key = key.replace('>', '%3E').replace('<', '%3C')
                    # åªå¯¹å€¼ä¸­çš„é€—å·è¿›è¡Œç¼–ç 
                    encoded_value = str(value).replace(',', '%2C')
                    api_url += f"&{encoded_key}={encoded_value}"
            
            logger.info(f"è¯·æ±‚URL: {api_url}")
            
            logger.info(f"è¯·æ±‚ç¬¬ {offset//limit + 1} é¡µæ•°æ®ï¼Œoffset: {offset}")
            
            # æ·»åŠ é‡è¯•æœºåˆ¶ï¼Œå‚è€ƒæ–‡ä»¶ä¸­çš„å®ç°
            max_retries = 100
            retry_delay = 10  # ç§’
            
            for attempt in range(max_retries):
                try:
                    # æ„å»ºåŸºæœ¬çš„è¯·æ±‚å¤´
                    headers = {
                        'Accept': 'application/json, text/plain, */*',
                        'Accept-Language': 'zh-CN,zh;q=0.9',
                        'Cache-Control': 'no-cache',
                        'Pragma': 'no-cache',
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36'
                    }
                    
                    response = self.session.get(api_url, headers=headers, timeout=30)
                    
                    if response.status_code == 200:
                        data = response.json()
                        count = data.get('count', 0)
                        results_count = len(data.get('results', []))
                        logger.info(f"è·å–æˆåŠŸï¼Œæ€»æ•°: {count}, æœ¬é¡µæ•°é‡: {results_count}")
                        return data
                    elif response.status_code == 429:
                        retry_after = int(response.headers.get("Retry-After", retry_delay))
                        logger.warning(f"APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œ{retry_after}ç§’åé‡è¯•...")
                        time.sleep(retry_after)
                        continue
                    else:
                        logger.error(f"è·å–æ•°æ®å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                        # æ‰“å°å“åº”å†…å®¹ç”¨äºè°ƒè¯•
                        logger.error(f"å“åº”å†…å®¹: {response.text[:500]}")
                        if attempt < max_retries - 1:
                            logger.info(f"{retry_delay}ç§’åé‡è¯•...")
                            time.sleep(retry_delay)
                            continue
                        else:
                            return None
                            
                except Exception as e:
                    logger.error(f"è·å–æ•°æ®è¯·æ±‚å¼‚å¸¸ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                    if attempt < max_retries - 1:
                        logger.info(f"{retry_delay}ç§’åé‡è¯•...")
                        time.sleep(retry_delay)
                        continue
                    else:
                        return None
            
            return None
                
        except Exception as e:
            logger.error(f"è·å–æ•°æ®è¯·æ±‚å¼‚å¸¸: {e}")
            return None
    
    def crawl_alphas(self, total_limit: Optional[int] = None, 
                    filters: Optional[Dict] = None, resume_from: int = None, 
                    task_id: str = 'default', crawl_status_id: Optional[int] = None) -> bool:
        """çˆ¬å–Alphaæ•°æ® - ä¸€é¡µä¸€å…¥åº“ï¼Œæ”¯æŒæ–­ç‚¹ç»­è¿
        
        Args:
            total_limit: æ€»æ•°é™åˆ¶
            filters: è¿‡æ»¤æ¡ä»¶
            resume_from: æ–­ç‚¹ç»­è¿èµ·å§‹ä½ç½®
            task_id: ä»»åŠ¡IDï¼Œç”¨äºçŠ¶æ€è®°å½•
            crawl_status_id: æ‰¹æ¬¡è®°å½•IDï¼Œå¦‚æœæä¾›åˆ™ç›´æ¥ä½¿ç”¨è¯¥è®°å½•
        """
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = datetime.now()
        
        # æ–­ç‚¹ç»­è¿é€»è¾‘
        if resume_from is not None:
            offset = resume_from
            logger.info(f"æ–­ç‚¹ç»­è¿: ä»offset {resume_from}å¼€å§‹çˆ¬å–... å¼€å§‹æ—¶é—´: {start_time}")
        else:
            offset = 0
            logger.info(f"å¼€å§‹çˆ¬å–Alphaæ•°æ®... å¼€å§‹æ—¶é—´: {start_time}")
        
        # ä½¿ç”¨æä¾›çš„æ‰¹æ¬¡è®°å½•IDæˆ–æŸ¥æ‰¾ç°æœ‰è®°å½•
        if crawl_status_id is not None:
            # ç›´æ¥ä½¿ç”¨æä¾›çš„æ‰¹æ¬¡è®°å½•ID
            logger.info(f"ä½¿ç”¨æä¾›çš„æ‰¹æ¬¡è®°å½• ID: {crawl_status_id}")
            # æ›´æ–°è®°å½•çŠ¶æ€ä¸ºrunningï¼Œå¹¶è®¡ç®—duration_secondsï¼ˆä»start_timeåˆ°å½“å‰æ—¶é—´ï¼‰
            cursor = self.db_connection.cursor()
            sql = "UPDATE crawl_status SET status = 'running', start_time = %s, duration_seconds = TIMESTAMPDIFF(SECOND, %s, NOW()) WHERE id = %s"
            cursor.execute(sql, (start_time, start_time, crawl_status_id))
            self.db_connection.commit()
            cursor.close()
        else:
            # å¦‚æœæ²¡æœ‰æä¾›æ‰¹æ¬¡è®°å½•IDï¼Œåˆ™æŸ¥æ‰¾ç°æœ‰è®°å½•
            cursor = self.db_connection.cursor()
            sql = "SELECT id FROM crawl_status WHERE task_id = %s AND batch_info = %s AND status = 'pending'"
            batch_info_json = json.dumps(filters, ensure_ascii=False) if filters else None
            cursor.execute(sql, (task_id, batch_info_json))
            result = cursor.fetchone()
            cursor.close()
            
            if result:
                crawl_status_id = result[0]
                # æ›´æ–°è®°å½•çŠ¶æ€ä¸ºrunningï¼Œå¹¶è®¡ç®—duration_secondsï¼ˆä»start_timeåˆ°å½“å‰æ—¶é—´ï¼‰
                cursor = self.db_connection.cursor()
                sql = "UPDATE crawl_status SET status = 'running', start_time = %s, duration_seconds = TIMESTAMPDIFF(SECOND, %s, NOW()) WHERE id = %s"
                cursor.execute(sql, (start_time, start_time, crawl_status_id))
                self.db_connection.commit()
                cursor.close()
                logger.info(f"ä½¿ç”¨ç°æœ‰æ‰¹æ¬¡è®°å½• ID: {crawl_status_id}")
            else:
                logger.warning(f"æœªæ‰¾åˆ°åŒ¹é…çš„æ‰¹æ¬¡è®°å½•ï¼Œå°†åˆ›å»ºæ–°è®°å½•")
                crawl_status_id = self.create_crawl_status(start_time, filters, task_id)
        
        # åˆå§‹åŒ–ç»Ÿè®¡
        total_count = 0
        success_count = 0
        error_count = 0
        limit = 100
        
        try:
            while True:
                # æ£€æŸ¥è®¤è¯çŠ¶æ€ï¼Œå¦‚æœè¿‡æœŸåˆ™é‡æ–°è®¤è¯
                if not self.authenticate():
                    logger.error("è®¤è¯å¤±è´¥ï¼Œå°è¯•é‡æ–°è®¤è¯")
                    # é‡æ–°è®¤è¯å¤±è´¥åˆ™ç»§ç»­å°è¯•ï¼Œä¸åœæ­¢çˆ¬å–
                    continue
                
                # è·å–ä¸€é¡µæ•°æ®
                page_data = self.get_alphas_page(limit, offset, filters)
                
                if not page_data:
                    logger.error(f"ç¬¬ {offset//limit + 1} é¡µæ•°æ®è·å–å¤±è´¥")
                    error_count += 1
                    break
                
                results = page_data.get('results', [])
                if not results:
                    logger.info("æ²¡æœ‰æ›´å¤šæ•°æ®")
                    break
                
                # å¤„ç†æœ¬é¡µæ•°æ®
                page_success = 0
                page_error = 0
                
                for alpha_data in results:
                    try:
                        # è§£ææ•°æ®
                        parsed_data = self.parse_alpha_data(alpha_data)
                        
                        if not parsed_data:
                            logger.warning(f"æ•°æ®è§£æå¤±è´¥: {alpha_data.get('id')}")
                            page_error += 1
                            continue
                        
                        # ä¿å­˜åˆ°æ•°æ®åº“
                        if self.save_alpha_to_database(parsed_data):
                            page_success += 1
                        else:
                            page_error += 1
                            
                    except Exception as e:
                        logger.error(f"å¤„ç†Alphaæ•°æ®å¼‚å¸¸: {e}")
                        page_error += 1
                
                # æ›´æ–°ç»Ÿè®¡
                success_count += page_success
                error_count += page_error
                total_count += len(results)
                
                logger.info(f"ç¬¬ {offset//limit + 1} é¡µå¤„ç†å®Œæˆ: æˆåŠŸ {page_success}, å¤±è´¥ {page_error}")
                
                # æ›´æ–°çˆ¬è™«çŠ¶æ€è®°å½•
                self.update_crawl_status(crawl_status_id, total_count, success_count, error_count, offset)
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ€»æ•°é™åˆ¶
                if total_limit and total_count >= total_limit:
                    logger.info(f"è¾¾åˆ°æ€»æ•°é™åˆ¶ {total_limit}ï¼Œåœæ­¢çˆ¬å–")
                    break
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
                next_url = page_data.get('next')
                if not next_url:
                    logger.info("å·²è·å–æ‰€æœ‰æ•°æ®")
                    break
                
                # æ›´æ–°offset
                offset += limit
                
                # æ·»åŠ å»¶è¿Ÿé¿å…é¢‘ç¹è¯·æ±‚
                time.sleep(2)
            
            # è®°å½•ç»“æŸæ—¶é—´
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            # æ›´æ–°çˆ¬è™«çŠ¶æ€è®°å½•ä¸ºå®ŒæˆçŠ¶æ€
            self.complete_crawl_status(crawl_status_id, total_count, success_count, error_count, offset, end_time, duration)
            
            logger.info(f"çˆ¬å–å®Œæˆ: æ€»æ•° {total_count}, æˆåŠŸ {success_count}, å¤±è´¥ {error_count}, è€—æ—¶ {duration} ç§’")
            return success_count > 0
            
        except Exception as e:
            # è®°å½•é”™è¯¯ä¿¡æ¯
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            error_message = str(e)
            
            # æ›´æ–°çˆ¬è™«çŠ¶æ€è®°å½•ä¸ºé”™è¯¯çŠ¶æ€
            self.error_crawl_status(crawl_status_id, total_count, success_count, error_count, offset, end_time, duration, error_message)
            
            logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
            return False
    
    def save_crawl_status(self, total_count: int, success_count: int, error_count: int, 
                         last_offset: int, task_id: str = 'default', task_type: str = 'alpha_crawl',
                         task_params: Optional[Dict] = None) -> bool:
        """ä¿å­˜çˆ¬è™«çŠ¶æ€"""
        try:
            cursor = self.db_connection.cursor()
            
            # å°†ä»»åŠ¡å‚æ•°è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
            task_params_json = json.dumps(task_params, ensure_ascii=False) if task_params else None
            
            sql = """
            INSERT INTO crawl_status (crawl_date, total_count, success_count, error_count, last_offset, status,
                                     task_id, task_type, task_params)
            VALUES (CURDATE(), %s, %s, %s, %s, %s, %s, %s, %s)
            """
            
            cursor.execute(sql, (total_count, success_count, error_count, last_offset, 'completed',
                               task_id, task_type, task_params_json))
            self.db_connection.commit()
            cursor.close()
            
            logger.info("çˆ¬è™«çŠ¶æ€ä¿å­˜æˆåŠŸ")
            return True
            
        except Error as e:
            logger.error(f"ä¿å­˜çˆ¬è™«çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def create_crawl_status(self, start_time: datetime, filters: Optional[Dict] = None,
                           task_id: str = 'default', task_type: str = 'alpha_crawl',
                           task_params: Optional[Dict] = None) -> Optional[int]:
        """åˆ›å»ºæ–°çš„çˆ¬è™«çŠ¶æ€è®°å½•"""
        try:
            cursor = self.db_connection.cursor()
            
            # å°†è¿‡æ»¤æ¡ä»¶å’Œä»»åŠ¡å‚æ•°è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
            batch_info = json.dumps(filters, ensure_ascii=False) if filters else None
            task_params_json = json.dumps(task_params, ensure_ascii=False) if task_params else None
            
            sql = """
            INSERT INTO crawl_status (crawl_date, total_count, success_count, error_count, last_offset, status, 
                                     start_time, batch_info, task_id, task_type, task_params)
            VALUES (CURDATE(), %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """
            
            cursor.execute(sql, (0, 0, 0, 0, 'running', start_time, batch_info, task_id, task_type, task_params_json))
            self.db_connection.commit()
            
            # è·å–æ’å…¥è®°å½•çš„ID
            crawl_status_id = cursor.lastrowid
            cursor.close()
            
            logger.info(f"åˆ›å»ºçˆ¬è™«çŠ¶æ€è®°å½•æˆåŠŸï¼ŒID: {crawl_status_id}, ä»»åŠ¡å·: {task_id}")
            return crawl_status_id
            
        except Error as e:
            logger.error(f"åˆ›å»ºçˆ¬è™«çŠ¶æ€è®°å½•å¤±è´¥: {e}")
            return None
    
    def update_crawl_status(self, crawl_status_id: int, total_count: int, success_count: int, 
                           error_count: int, last_offset: int) -> bool:
        """æ›´æ–°çˆ¬è™«çŠ¶æ€è®°å½•"""
        try:
            cursor = self.db_connection.cursor()
            
            sql = """
            UPDATE crawl_status 
            SET total_count = %s, success_count = %s, error_count = %s, last_offset = %s, 
                duration_seconds = TIMESTAMPDIFF(SECOND, start_time, NOW()), updated_at = NOW()
            WHERE id = %s
            """
            
            cursor.execute(sql, (total_count, success_count, error_count, last_offset, crawl_status_id))
            self.db_connection.commit()
            cursor.close()
            
            return True
            
        except Error as e:
            logger.error(f"æ›´æ–°çˆ¬è™«çŠ¶æ€è®°å½•å¤±è´¥: {e}")
            return False
    
    def complete_crawl_status(self, crawl_status_id: int, total_count: int, success_count: int, 
                             error_count: int, last_offset: int, end_time: datetime, 
                             duration: float) -> bool:
        """å®Œæˆçˆ¬è™«çŠ¶æ€è®°å½•"""
        try:
            cursor = self.db_connection.cursor()
            
            sql = """
            UPDATE crawl_status 
            SET total_count = %s, success_count = %s, error_count = %s, last_offset = %s, 
                status = %s, end_time = %s, duration_seconds = %s, updated_at = NOW()
            WHERE id = %s
            """
            
            cursor.execute(sql, (total_count, success_count, error_count, last_offset, 
                               'completed', end_time, duration, crawl_status_id))
            self.db_connection.commit()
            cursor.close()
            
            logger.info(f"çˆ¬è™«çŠ¶æ€è®°å½•æ›´æ–°ä¸ºå®ŒæˆçŠ¶æ€ï¼ŒID: {crawl_status_id}")
            return True
            
        except Error as e:
            logger.error(f"å®Œæˆçˆ¬è™«çŠ¶æ€è®°å½•å¤±è´¥: {e}")
            return False
    
    def error_crawl_status(self, crawl_status_id: int, total_count: int, success_count: int, 
                          error_count: int, last_offset: int, end_time: datetime, 
                          duration: float, error_message: str) -> bool:
        """æ ‡è®°çˆ¬è™«çŠ¶æ€è®°å½•ä¸ºé”™è¯¯çŠ¶æ€"""
        try:
            cursor = self.db_connection.cursor()
            
            sql = """
            UPDATE crawl_status 
            SET total_count = %s, success_count = %s, error_count = %s, last_offset = %s, 
                status = %s, end_time = %s, duration_seconds = %s, error_message = %s, updated_at = NOW()
            WHERE id = %s
            """
            
            cursor.execute(sql, (total_count, success_count, error_count, last_offset, 
                               'error', end_time, duration, error_message, crawl_status_id))
            self.db_connection.commit()
            cursor.close()
            
            logger.info(f"çˆ¬è™«çŠ¶æ€è®°å½•æ›´æ–°ä¸ºé”™è¯¯çŠ¶æ€ï¼ŒID: {crawl_status_id}")
            return True
            
        except Error as e:
            logger.error(f"é”™è¯¯çˆ¬è™«çŠ¶æ€è®°å½•å¤±è´¥: {e}")
            return False
    
    def get_latest_successful_crawl(self, task_type: str = 'alpha_crawl') -> Optional[Dict]:
        """è·å–æœ€è¿‘ä¸€æ¬¡æˆåŠŸçš„çˆ¬è™«çŠ¶æ€ï¼ˆerror_countä¸º0ï¼‰"""
        try:
            cursor = self.db_connection.cursor()
            
            sql = """
            SELECT id, crawl_date, start_time, end_time, total_count, success_count, 
                   error_count, last_offset, status, task_id, task_type, task_params
            FROM crawl_status 
            WHERE error_count = 0 AND status = 'completed' AND task_type = %s
            ORDER BY end_time DESC 
            LIMIT 1
            """
            
            cursor.execute(sql, (task_type,))
            result = cursor.fetchone()
            cursor.close()
            
            if result:
                return {
                    'id': result[0],
                    'crawl_date': result[1],
                    'start_time': result[2],
                    'end_time': result[3],
                    'total_count': result[4],
                    'success_count': result[5],
                    'error_count': result[6],
                    'last_offset': result[7],
                    'status': result[8],
                    'task_id': result[9],
                    'task_type': result[10],
                    'task_params': json.loads(result[11]) if result[11] else None
                }
            return None
            
        except Error as e:
            logger.error(f"è·å–æœ€è¿‘æˆåŠŸçˆ¬è™«çŠ¶æ€å¤±è´¥: {e}")
            return None
    
    def generate_task_id(self, task_type: str = 'alpha_crawl', 
                        start_date: str = None, 
                        end_date: str = None,
                        fitness_ranges: List[str] = None) -> str:
        """ç”Ÿæˆä»»åŠ¡å·ï¼ŒåŒ…å«ä»»åŠ¡å±æ€§åŠç¼–ç """
        from datetime import datetime
        import hashlib
        
        # åŸºç¡€ä¿¡æ¯
        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
        
        # æ„å»ºä»»åŠ¡å±æ€§å­—ç¬¦ä¸²
        task_attrs = f"{task_type}_{start_date or 'all'}_{end_date or 'all'}"
        if fitness_ranges:
            fitness_str = '_'.join([str(r) for r in fitness_ranges[:3]])  # å–å‰3ä¸ªèŒƒå›´
            task_attrs += f"_{fitness_str}"
        
        # ç”Ÿæˆå”¯ä¸€ç¼–ç 
        unique_str = f"{task_attrs}_{timestamp}"
        hash_code = hashlib.md5(unique_str.encode()).hexdigest()[:8]
        
        # ç»„åˆä»»åŠ¡å·
        task_id = f"{task_type}_{timestamp}_{hash_code}"
        
        logger.info(f"ç”Ÿæˆä»»åŠ¡å·: {task_id}")
        return task_id

    def close(self):
        """å…³é—­è¿æ¥"""
        if self.db_connection and self.db_connection.is_connected():
            self.db_connection.close()
            logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")

    def create_daily_batch_filters(self, start_date: str, end_date: str, 
                                 fitness_ranges: Optional[List[Tuple[str, str, str]]] = None,
                                 additional_filters: Optional[Dict] = None) -> List[Dict]:
        """
        åˆ›å»ºæ¯æ—¥åˆ†æ‰¹è¿‡æ»¤æ¡ä»¶ï¼Œæ”¯æŒfitnessèŒƒå›´å’Œå…¶ä»–æ¡ä»¶
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            fitness_ranges: fitnessèŒƒå›´é…ç½®ï¼Œæ ¼å¼ä¸º[(range_name, min_value, max_value), ...]
            additional_filters: é¢å¤–çš„è¿‡æ»¤æ¡ä»¶
            
        Returns:
            æ¯æ—¥åˆ†æ‰¹è¿‡æ»¤æ¡ä»¶åˆ—è¡¨
        """
        try:
            # è§£ææ—¥æœŸèŒƒå›´
            start_dt = datetime.strptime(start_date, '%Y-%m-%d')
            end_dt = datetime.strptime(end_date, '%Y-%m-%d')
            
            # è®¡ç®—å¤©æ•°
            delta = end_dt - start_dt
            total_days = delta.days + 1
            
            if total_days <= 0:
                logger.error("æ—¥æœŸèŒƒå›´æ— æ•ˆ")
                return []
            
            # é»˜è®¤fitnessèŒƒå›´é…ç½®
            if fitness_ranges is None:
                fitness_ranges = [
                    ('<-1.0', None, '-1.0'),
                    ('-1.0-0.5', '-1.0', '-0.5'),
                    ('-0.5-0.3', '-0.5', '0.3'),
                    ('-0.3-0', '-0.3', '0'),
                    ('0-0.3', '0', '0.3'),
                    ('0.3-0.5', '0.3', '0.5'),
                    ('0.5-0.7', '0.5', '0.7'),
                    ('0.7-1.0', '0.7', '1.0'),
                    ('1.0-1.5', '1.0', '1.5'),
                    ('1.5-2.0', '1.5', '2.0'),
                    ('2.0-3.0', '2.0', '3.0'),
                    ('3.0-4.0', '3.0', '4.0'),
                    ('>=4.0', '4.0', None)
                ]
            
            # ä½¿ç”¨ç±»çº§åˆ«çš„åŸºç¡€è¿‡æ»¤æ¡ä»¶
            base_filters = self.base_filters
            
            # åˆå¹¶é¢å¤–çš„è¿‡æ»¤æ¡ä»¶
            if additional_filters:
                base_filters.update(additional_filters)
            
            batch_filters = []
            
            # ä¸ºæ¯ä¸€å¤©åˆ›å»ºfitnessèŒƒå›´æ‰¹æ¬¡
            for day_offset in range(total_days):
                current_date = start_dt + timedelta(days=day_offset)
                date_str = current_date.strftime('%Y-%m-%d')
                
                # ä¸ºæ¯ä¸ªfitnessèŒƒå›´åˆ›å»ºè¿‡æ»¤æ¡ä»¶
                for range_name, min_val, max_val in fitness_ranges:
                    filters = base_filters.copy()
                    
                    # æ·»åŠ æ—¥æœŸè¿‡æ»¤æ¡ä»¶ï¼ˆä½¿ç”¨æ­£ç¡®çš„æ—¶åŒºæ ¼å¼ï¼‰
                    filters[f'dateCreated>='] = f'{date_str}T00:00:00-04:00'
                    # ç»“æŸæ—¶é—´åº”è¯¥æ˜¯ä¸‹ä¸€å¤©çš„00:00ï¼Œè€Œä¸æ˜¯å½“å¤©çš„23:59
                    next_date = current_date + timedelta(days=1)
                    next_date_str = next_date.strftime('%Y-%m-%d')
                    filters[f'dateCreated<'] = f'{next_date_str}T00:00:00-04:00'
                    
                    # æ·»åŠ fitnessè¿‡æ»¤æ¡ä»¶
                    if min_val is not None and max_val is not None:
                        filters['is.fitness>='] = min_val
                        filters['is.fitness<'] = max_val
                        fitness_desc = f"fitness>={min_val} AND fitness<{max_val}"
                    elif min_val is not None:
                        filters['is.fitness>='] = min_val
                        fitness_desc = f"fitness>={min_val}"
                    elif max_val is not None:
                        filters['is.fitness<'] = max_val
                        fitness_desc = f"fitness<{max_val}"
                    else:
                        fitness_desc = "æ— fitnessé™åˆ¶"
                    
                    batch_filters.append({
                        'filters': filters,
                        'date': date_str,
                        'fitness_range': range_name,
                        'fitness_min': min_val,
                        'fitness_max': max_val,
                        'description': f"{date_str} - {range_name} ({fitness_desc})"
                    })
            
            logger.info(f"åˆ›å»ºäº† {len(batch_filters)} ä¸ªåˆ†æ‰¹è¿‡æ»¤æ¡ä»¶ï¼Œè¦†ç›– {total_days} å¤©ï¼Œ{len(fitness_ranges)} ä¸ªfitnessèŒƒå›´")
            return batch_filters
            
        except Exception as e:
            logger.error(f"åˆ›å»ºåˆ†æ‰¹è¿‡æ»¤æ¡ä»¶å¤±è´¥: {e}")
            return []
    


def initialize_crawler() -> Optional[AlphaCrawler]:
    """åˆå§‹åŒ–çˆ¬è™«å®ä¾‹"""
    crawler = AlphaCrawler()
    
    # 1. APIè®¤è¯
    logger.info("æ­¥éª¤1: APIè®¤è¯")
    if not crawler.authenticate():
        logger.error("APIè®¤è¯å¤±è´¥")
        return None
    
    # 2. è¿æ¥æ•°æ®åº“
    logger.info("æ­¥éª¤2: è¿æ¥æ•°æ®åº“")
    if not crawler.connect_database():
        logger.error("æ•°æ®åº“è¿æ¥å¤±è´¥")
        return None
    
    # 3. åˆ›å»ºæ•°æ®åº“è¡¨
    logger.info("æ­¥éª¤3: åˆ›å»ºæ•°æ®åº“è¡¨")
    if not crawler.create_tables():
        logger.error("æ•°æ®åº“è¡¨åˆ›å»ºå¤±è´¥")
        return None
    
    return crawler

def check_resume_point(crawler: AlphaCrawler, resume: bool) -> Tuple[str, bool]:
    """æ£€æŸ¥æ–­ç‚¹ç»­è¿ç‚¹
    
    Args:
        crawler: çˆ¬è™«å®ä¾‹
        resume: æ˜¯å¦å¼€å¯æ–­ç‚¹ç»­è¿
        
    Returns:
        (å®é™…ä»»åŠ¡ID, æ˜¯å¦ç»§ç»­å¤„ç†ç°æœ‰ä»»åŠ¡)
    """
    actual_task_id = f"alpha_crawl_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    if not resume:
        logger.info("æ–­ç‚¹ç»­è¿æ¨¡å¼å·²ç¦ç”¨ï¼Œä½¿ç”¨æ–°ä»»åŠ¡")
        return actual_task_id, False
    
    # æŸ¥æ‰¾å‰ä¸€ä¸ªä¸»ä»»åŠ¡IDï¼ˆä¸æ’é™¤å½“å‰ä»»åŠ¡IDï¼Œå› ä¸ºæ­¤æ—¶è¿˜æ²¡æœ‰è®°å½•ï¼‰
    cursor = crawler.db_connection.cursor()
    sql = """
    SELECT DISTINCT task_id, created_at FROM crawl_status 
    WHERE task_type = 'alpha_crawl_batch' 
    ORDER BY created_at DESC 
    LIMIT 1
    """
    cursor.execute(sql)
    result = cursor.fetchone()
    cursor.close()
    
    if not result:
        logger.info("æ–­ç‚¹ç»­è¿ï¼šæœªæ‰¾åˆ°å‰ä¸€ä¸ªä¸»ä»»åŠ¡ï¼Œä½¿ç”¨æ–°ä»»åŠ¡")
        return actual_task_id, False
    
    previous_task_id = result[0]
    # æ£€æŸ¥å‰ä¸€ä¸ªä¸»ä»»åŠ¡çš„æ‰€æœ‰æ‰¹æ¬¡æ˜¯å¦éƒ½å·²å®Œæˆï¼ˆåªæœ‰completedçŠ¶æ€æ‰ç®—å®Œæˆï¼ŒrunningçŠ¶æ€éœ€è¦é‡æ–°å¤„ç†ï¼‰
    cursor = crawler.db_connection.cursor()
    sql = """
    SELECT COUNT(*) as total_batches,
           SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as completed_batches,
           SUM(CASE WHEN status = 'running' THEN 1 ELSE 0 END) as running_batches
    FROM crawl_status 
    WHERE task_id = %s AND task_type = 'alpha_crawl_batch'
    """
    cursor.execute(sql, (previous_task_id,))
    result = cursor.fetchone()
    cursor.close()
    
    if result and result[0] > 0 and result[1] < result[0]:
        # å‰ä¸€ä¸ªä¸»ä»»åŠ¡æœ‰æœªå®Œæˆçš„æ‰¹æ¬¡ï¼ˆåªæœ‰completedçŠ¶æ€æ‰ç®—å®Œæˆï¼ŒrunningçŠ¶æ€éœ€è¦é‡æ–°å¤„ç†ï¼‰ï¼Œç»§ç»­å¤„ç†å‰ä¸€ä¸ªä»»åŠ¡
        actual_task_id = previous_task_id
        logger.info(f"æ–­ç‚¹ç»­è¿ï¼šæ£€æµ‹åˆ°å‰ä¸€ä¸ªä¸»ä»»åŠ¡ {previous_task_id} æœ‰æœªå®Œæˆæ‰¹æ¬¡ï¼Œç»§ç»­å¤„ç†è¯¥ä»»åŠ¡")
        logger.info(f"æ‰¹æ¬¡å®Œæˆæƒ…å†µï¼š{result[1]}/{result[0]} å·²å®Œæˆï¼Œ{result[2]}/{result[0]} è¿è¡Œä¸­")
        return actual_task_id, True
    else:
        logger.info(f"æ–­ç‚¹ç»­è¿ï¼šå‰ä¸€ä¸ªä¸»ä»»åŠ¡ {previous_task_id} æ‰€æœ‰æ‰¹æ¬¡å·²å®Œæˆï¼Œä½¿ç”¨æ–°ä»»åŠ¡ {actual_task_id}")
        return actual_task_id, False

def process_batch_data(crawler: AlphaCrawler, task_id: str, total_limit: int) -> Tuple[int, int]:
    """å¤„ç†åˆ†æ‰¹æ•°æ®
    
    Args:
        crawler: çˆ¬è™«å®ä¾‹
        task_id: ä»»åŠ¡ID
        total_limit: æ¯æ‰¹æ•°æ®é‡é™åˆ¶
        
    Returns:
        (æˆåŠŸæ‰¹æ¬¡æ•°é‡, å¤±è´¥æ‰¹æ¬¡æ•°é‡)
    """
    total_success = 0
    total_error = 0
    
    # ä»æ•°æ®åº“è¯»å–æ‰¹æ¬¡ä¿¡æ¯ï¼Œä¼˜å…ˆå¤„ç†runningçŠ¶æ€çš„æ‰¹æ¬¡ï¼Œç„¶åæ˜¯pendingçŠ¶æ€çš„æ‰¹æ¬¡
    cursor = crawler.db_connection.cursor()
    sql = """
    SELECT id, batch_info, status FROM crawl_status 
    WHERE task_id = %s AND (status = 'running' OR status = 'pending') 
    ORDER BY 
        CASE WHEN status = 'running' THEN 1 ELSE 2 END,
        id
    """
    cursor.execute(sql, (task_id,))
    batch_records = cursor.fetchall()
    cursor.close()
    
    if not batch_records:
        logger.warning(f"æœªæ‰¾åˆ°ä»»åŠ¡ {task_id} çš„å¾…å¤„ç†æ‰¹æ¬¡è®°å½•")
        return 0, 0
    
    logger.info(f"ä»æ•°æ®åº“è¯»å–åˆ° {len(batch_records)} ä¸ªå¾…å¤„ç†æ‰¹æ¬¡")
    
    for i, record in enumerate(batch_records):
            
        batch_info = json.loads(record[1])  # batch_infoå­—æ®µ
        
        logger.info(f"=== å¼€å§‹å¤„ç†ç¬¬ {i+1} æ‰¹æ•°æ® ===")
        logger.info(f"æ‰¹æ¬¡æè¿°: {batch_info.get('description', 'æœªçŸ¥æ‰¹æ¬¡')}")
        logger.info(f"è¿‡æ»¤æ¡ä»¶: {batch_info.get('filters', {})}")
        
        # ä½¿ç”¨ä»æ•°æ®åº“è¯»å–çš„æ‰¹æ¬¡ä¿¡æ¯è¿›è¡Œçˆ¬å–ï¼Œç›´æ¥ä¼ é€’æ‰¹æ¬¡è®°å½•ID
        success = crawler.crawl_alphas(total_limit=total_limit, filters=batch_info.get('filters', {}), task_id=task_id, crawl_status_id=record[0])
        
        if success:
            logger.info(f"ç¬¬ {i+1} æ‰¹æ•°æ®çˆ¬å–æˆåŠŸ")
            total_success += 1
        else:
            logger.error(f"ç¬¬ {i+1} æ‰¹æ•°æ®çˆ¬å–å¤±è´¥")
            total_error += 1
        
        # æ‰¹æ¬¡é—´å»¶è¿Ÿ
        time.sleep(5)
    
    return total_success, total_error

def main(start_date: str = "2025-08-28", 
          end_date: str = None,
          total_limit: int = 10000,
          resume: bool = True):
    """ä¸»å‡½æ•°ï¼Œæ”¯æŒæ–­ç‚¹ç»­è¿
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼: 2025-05-10
        end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼: 2025-10-24ï¼Œå¦‚æœä¸ºNoneåˆ™åŠ¨æ€è·å–æ˜å¤©æ—¥æœŸ
        total_limit: æ¯æ‰¹æ•°æ®é‡é™åˆ¶
        resume: æ˜¯å¦æ–­ç‚¹ç»­è¿ï¼Œé»˜è®¤å¼€å¯
    """
    # å¦‚æœend_dateä¸ºNoneï¼Œåˆ™åŠ¨æ€è®¡ç®—æ˜å¤©çš„æ—¥æœŸ
    if end_date is None:
        from datetime import datetime, timedelta
        tomorrow = (datetime.now() + timedelta(days=1)).strftime('%Y-%m-%d')
        end_date = tomorrow
        logger.info(f"ä½¿ç”¨åŠ¨æ€è®¡ç®—çš„ç»“æŸæ—¥æœŸ: {end_date}")
    
    logger.info("=== WorldQuant Alphaæ•°æ®çˆ¬è™«å¯åŠ¨ ===")
    logger.info(f"æ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
    logger.info(f"æ–­ç‚¹ç»­è¿æ¨¡å¼: {'å¯ç”¨' if resume else 'ç¦ç”¨'}")
    
    # åˆå§‹åŒ–çˆ¬è™«
    crawler = initialize_crawler()
    if not crawler:
        logger.error("çˆ¬è™«åˆå§‹åŒ–å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return False
    
    try:
        # æ–­ç‚¹ç»­è¿æ£€æŸ¥
        actual_task_id, should_resume = check_resume_point(crawler, resume)
        
        if should_resume:
            # ç›´æ¥è¿›å…¥æ­¥éª¤6ï¼Œè·³è¿‡æ­¥éª¤4å’Œ5ï¼ˆæ–­ç‚¹ç»­è¿æ¨¡å¼ï¼‰
            logger.info("æ­¥éª¤6: å¼€å§‹åˆ†æ‰¹çˆ¬å–æ•°æ®ï¼ˆæ–­ç‚¹ç»­è¿æ¨¡å¼ï¼‰")
            total_success, total_error = process_batch_data(crawler, actual_task_id, total_limit)
            
            # è¾“å‡ºç»“æœ
            if total_success > 0:
                logger.info(f"=== çˆ¬è™«æ‰§è¡ŒæˆåŠŸï¼ŒæˆåŠŸæ‰¹æ¬¡: {total_success}, å¤±è´¥æ‰¹æ¬¡: {total_error} ===")
            else:
                logger.error(f"=== çˆ¬è™«æ‰§è¡Œå¤±è´¥ï¼ŒæˆåŠŸæ‰¹æ¬¡: {total_success}, å¤±è´¥æ‰¹æ¬¡: {total_error} ===")
            
            return total_success > 0
        
        # å¦‚æœæ²¡æœ‰æ–­ç‚¹ç»­è¿æˆ–å‰ä¸€ä¸ªä»»åŠ¡å·²å®Œæˆï¼Œåˆ™æ­£å¸¸æ‰§è¡Œæ­¥éª¤4å’Œ5
        # åˆ›å»ºæ¯æ—¥åˆ†æ‰¹è¿‡æ»¤æ¡ä»¶
        logger.info("æ­¥éª¤4: åˆ›å»ºæ¯æ—¥åˆ†æ‰¹è¿‡æ»¤æ¡ä»¶")
        start_date_only = start_date.split('T')[0]
        end_date_only = end_date.split('T')[0]
        batch_filters = crawler.create_daily_batch_filters(start_date_only, end_date_only)
        
        if not batch_filters:
            logger.error("åˆ›å»ºåˆ†æ‰¹è¿‡æ»¤æ¡ä»¶å¤±è´¥")
            return False
        
        # å°†æ‰€æœ‰æ‰¹æ¬¡è¿‡æ»¤æ¡ä»¶å†™å…¥çˆ¬è™«è®°å½•è¡¨
        logger.info("æ­¥éª¤5: å°†æ‰¹æ¬¡è¿‡æ»¤æ¡ä»¶å†™å…¥çˆ¬è™«è®°å½•è¡¨")
        
        for i, batch_info in enumerate(batch_filters):
            batch_info_json = json.dumps(batch_info, ensure_ascii=False)
            
            # åˆ›å»ºæ‰¹æ¬¡è®°å½•åˆ°crawl_statusè¡¨
            cursor = crawler.db_connection.cursor()
            sql = """
            INSERT INTO crawl_status (crawl_date, total_count, success_count, error_count, last_offset, status, 
                                     batch_info, task_id, task_type, task_params)
            VALUES (CURDATE(), %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """
            cursor.execute(sql, (0, 0, 0, 0, 'pending', batch_info_json, actual_task_id, 'alpha_crawl_batch', None))
            crawler.db_connection.commit()
            cursor.close()
            
            logger.info(f"æ‰¹æ¬¡ {i+1} å·²è®°å½•åˆ°çˆ¬è™«è®°å½•è¡¨: {batch_info.get('description', 'æœªçŸ¥æ‰¹æ¬¡')}")
        
        logger.info(f"å…± {len(batch_filters)} ä¸ªæ‰¹æ¬¡è¿‡æ»¤æ¡ä»¶å·²å†™å…¥çˆ¬è™«è®°å½•è¡¨ï¼Œä¸»ä»»åŠ¡ID: {actual_task_id}")
        
        # åˆ†æ‰¹çˆ¬å–æ•°æ®
        logger.info("æ­¥éª¤6: å¼€å§‹åˆ†æ‰¹çˆ¬å–æ•°æ®")
        total_success, total_error = process_batch_data(crawler, actual_task_id, total_limit)
        
        # è¾“å‡ºç»“æœ
        if total_success > 0:
            logger.info(f"=== çˆ¬è™«æ‰§è¡ŒæˆåŠŸï¼ŒæˆåŠŸæ‰¹æ¬¡: {total_success}, å¤±è´¥æ‰¹æ¬¡: {total_error} ===")
        else:
            logger.error(f"=== çˆ¬è™«æ‰§è¡Œå¤±è´¥ï¼ŒæˆåŠŸæ‰¹æ¬¡: {total_success}, å¤±è´¥æ‰¹æ¬¡: {total_error} ===")
        
        return total_success > 0
        
    except Exception as e:
        logger.error(f"çˆ¬è™«æ‰§è¡Œå¼‚å¸¸: {e}")
        return False
    
    finally:
        # å…³é—­è¿æ¥
        crawler.close()
        logger.info("=== çˆ¬è™«æ‰§è¡Œç»“æŸ ===")

if __name__ == "__main__":
    # æ‰§è¡Œä¸»å‡½æ•°
    success = main()
    
    # æ ¹æ®æ‰§è¡Œç»“æœé€€å‡º
    sys.exit(0 if success else 1)
